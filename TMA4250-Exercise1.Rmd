--- 
title: "TMA4250 Spatial Statistics Exercise 1, Spring 2019"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  word_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: no
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Ã˜yvind Auestad (SKRIVE STUDENTNUMMER
  I STEDET??)'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(reshape2)
library(geoR)
library(ggplot2)
library(MASS)
library(cowplot)
library(fields)
library(akima)

```

# Problem 1: Gaussian RF - model characteristics
We consider the continuous spatial variable $\{r(x):x\in D : [1,50] \subset \mathbb{R}^1$, and assume that it is modeled as a stationary 1D Gaussian RF with the following model parameters:
\begin{gather*}
\text{E}\{r(x)\} = \mu_r = 0\\
\text{Var}\{r(x)\} = \sigma_r^2\\
\text{Corr}\{r(x), r(x')\} = \rho_r(\tau),
\end{gather*}
where $\rho_r(\tau)$; $\tau = \rvert x-x'\lvert/10$ is the spatial correlation function. Let $D:[1,50]$
be discretised in $L \in \{1, 2,\dots , 50\}$ and define the discretised Gaussian RF
$\{r(x); x \in L\}$, represented by the $n$-vector $\mathbf{r}\in\mathbb{R}^n$. 


Let the spatial correlation function $\rho_r(\tau)$, be either Powered exponential with parameter $\nu_r \in [1, 1.9]$ or Matern with parameter $\nu_r \in [1, 3]$. Let the variance take the values $\sigma_r^2 \in [1, 5]$.

## a)
We restrict the spatial correlation functions to be only positive definite functions. Covariance matrices need to be positive definite, and a positive definite correlation function ensures that all covariances matrices in our stationary Gaussian RF are positive definite for all configurations and dimensions. Further, we define a function $\rho(\tau):\mathbb{R}^q\rightarrow \mathbb{R}$ to be positive definite if
\begin{align*}
&\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j \rho(\mathbf{x}_i-\mathbf{x}_j) \geq 0,\\
&\text{for all configuations } [\mathbf{x}_1, \mathbf{x}_2,\ldots,\mathbf{x}_n]\in\mathbb{R}^{q\times n},\\
&\text{for all weights } \boldsymbol{\alpha} = (\alpha_1, \alpha_2,\ldots,\alpha_n)\in \mathbb{R}^n,\\
&\text{for all }n\in \mathbb{N}_+ \setminus \{1\}.
\end{align*}

To further investigate different correlation functions, we display two types of correlation functions, the Matern and Powered exponential functions, for different values of $\nu_r$. 

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
set.seed(123)
x = seq(0,5,length.out = 1001)
mat_1 = matern(x, 1, 1)
mat_2 = matern(x, 1, 3)
pexp_1 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1))
pexp_2 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1.9))
df = data.frame(y1 = mat_1, y2 = mat_2, y3 = pexp_1, y4 = pexp_2, value = x)
ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) +  
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(rho, "(", tau,")"))) + ggtitle(expression(paste("Display of two correlation functions for different values of ", nu)))
```

For all correlation functions $\rho_r(0)  =1$, and $\rho_r(\tau) \in [-1,1]$; $\tau\in\mathbb{R}_+$, because the function represents correlation between two random variables. The correlation functions are continuous everywhere, except at $\tau = 0$ where a step may occur. If the correlation function is continuous at $\tau = 0$, then the random field is continuous almost everywhere. Away from $\tau = 0$, the correlation function must be smooth. From the displayed correlation functions, we see that $\rho_r(\tau)\rightarrow 0$ as $\tau \rightarrow \infty$. This will be the case for all correlation functions, implying that two points $\mathbf{x}$ and $\mathbf{x}'$ will tend towards being uncorrelated as $\lvert\mathbf{x}-\mathbf{x}'\rvert\rightarrow\infty$. Uncorrelated Gaussian random fields means that they will be independent, and by extension asymptotics Gaussion random fields are ergodic.

The correlation functions define correlation between points in our random field. This means that larger values of the correlation function implies a smoother random field. From the plotted correlation function, we see that the function values increases for larger values of $\nu_r$. Consequently, larger values of $\nu_r$ implies smoother random fields for both these correlation functions.

We now define the variogram function, which for stationary Gaussian random fields is defined as $\gamma_r(\tau) = \sigma^2_r[1-\rho_r(\tau)]$. Then, we display the variogram functions associated with the previously plotted correlation functions for $\sigma_r^2 = 1$.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
gamma_1 = (1-mat_1)
gamma_2 = (1-mat_2)
gamma_3 = (1-pexp_1)
gamma_4 = (1-pexp_2)

df = data.frame(y1 = gamma_1, y2 = gamma_2, y3 = gamma_3, y4 = gamma_4, value = x)

ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) + 
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(gamma, "(", tau,")"))) + ggtitle(expression(paste("Variogram for two correlation functions for different values of ", nu)))+
  theme(legend.title=element_blank())
```

## b)
The prior Gaussion random field is defined on the discretized representation $L\in \{1,2,\dots,50\}$ by
$$\mathbf{r} \sim p(\mathbf{r}) = \phi_n(\mathbf{r};\mu_r \mathbf{i}_n, \sigma^2_r \boldsymbol{\Sigma}_r^\rho),$$
which is a discretized stationary Gaussian random field with expectation $\mu(\mathbf{x}) = \mu_r$, variance $\sigma^2(\mathbf{x}) = \sigma_r^2$ and correlation function $\rho(\mathbf{x},\mathbf{x}') = \rho_r(\mathbf{x}-\mathbf{x}')$.

We now want to simulate ten realizations of the Gaussian random field on $L$ for all the previously displayed correlation functions and $\sigma_r^2 \in [1,5]$.

```{r, echo = F, eval = T}
make_plot <- function(df,string){
  p <-ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = X1, col = "x1")) + 
    geom_line(aes(y = X2, col = "x2")) +
    geom_line(aes(y = X3, col = "x3")) + 
    geom_line(aes(y = X4, col = "x4")) + 
    geom_line(aes(y = X5, col = "x5")) + 
    geom_line(aes(y = X6, col = "x6")) +
    geom_line(aes(y = X7, col = "x7")) + 
    geom_line(aes(y = X8, col = "x8")) +    
    geom_line(aes(y = X9, col = "x9")) + 
    geom_line(aes(y = X10, col = "x10"))+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=11))  +
    ggtitle(string)
  return(p)
}

x = seq(1,50)
tau = abs(outer(x,x,'-')/10)
mu_r = rep(0,50)

sigmamatrix1 = matern(tau,1,1)
sample1 = mvrnorm(10,mu_r,sigmamatrix1)
df = data.frame(x, t(sample1))
p1 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=1")))

sigmamatrix2 = 5*matern(tau,1,1)
sample2 = mvrnorm(10,mu_r,sigmamatrix2)
df = data.frame(x, t(sample2))
p2 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=5")))

sigmamatrix3 = matern(tau,1,3)
sample3 = mvrnorm(10,mu_r,sigmamatrix3)
df = data.frame(x, t(sample3))
p3 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=1")))

sigmamatrix4 = 5*matern(tau,1,3)
sample4 = mvrnorm(10,mu_r,sigmamatrix4)
df = data.frame(x, t(sample4))
p4 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=5")))

sigmamatrix5 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample5 = mvrnorm(10,mu_r,sigmamatrix5)
df = data.frame(x, t(sample5))
p5 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=1")))

sigmamatrix6 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample6 = mvrnorm(10,mu_r,sigmamatrix6)
df = data.frame(x, t(sample6))
p6 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=5")))

sigmamatrix7 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample7 = mvrnorm(10,mu_r,sigmamatrix7)
df = data.frame(x, t(sample7))
p7 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=1")))

sigmamatrix8 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample8 = mvrnorm(10,mu_r,sigmamatrix8)
df = data.frame(x, t(sample8))
p8 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=5")))


plot_grid(p1,p2, p3, p4, p5, p6, p7, p8,nrow = 2, ncol = 4)
```

From the display, we see that a greater value of $\nu_r$ leads to smoother random fields. Also, a higher value of $\sigma_r^2$ leads to larger variations around the expected level $\mu_r = 0$. Also, the Matern correlation function leads to smoother random fields than the Powered exponential function.


## c)
We now observe the spatial variable as $\{d(x);x\in[10,25,30]\subset L\}$ according to the acquisition model,
$$d(x) = r(x) + \epsilon(x) \quad \quad x\in[10,25,30],$$
with measurement errors $\epsilon(\cdot)$ centred, i.i.d Gaussian with variance $\sigma_\epsilon^2$. Further, we assume that $r(x)$ and $\epsilon(x')$ are independent for all $x, x'$.

The likelihood model $p(\mathbf{d}|\mathbf{r})$ links the observations to the spatial variable. The observations $\mathbf{d}$ are known, while $\mathbf{r}$ is the spatial variable. This means that the likelihood $p(\mathbf{d}|\mathbf{r})$ is not a pdf w.r.t. $\mathbf{r}$, and we do not need to normalize the likelihood.

We can define a Gauss-linear likelihood model relative to the discretized spatial variable $\mathbf{r}$, and observe $\mathbf{d}\in\mathbb{R}^m$ according to
$$[\mathbf{d}|\mathbf{r}] = \mathbf{H}\mathbf{r} + \boldsymbol \epsilon_{d|r} \sim p(\mathbf{d}|\mathbf{r}) = \phi_m(\mathbf{d};\mathbf{H}\mathbf{r},\boldsymbol \Sigma_{d|r}),$$
where $\mathbf{H}$ is a $(m\times n)$ observation matrix. In our case, because we have three observations, $m = 3$. Also, the observations are assumed to have i.i.d. errors, which implies $\boldsymbol \Sigma_{d|r} = \sigma_\epsilon^2 I_{m\times m}$.

## d)
The pdf for the discretised posterior Gaussian random field, given the observations is defined in the following way,
$$[\mathbf{r}|\mathbf{d}]\sim p(\mathbf{r}|\mathbf{d}) = \phi_n(\mathbf{r};\boldsymbol \mu_{r|d},\boldsymbol \Sigma_{r|d},)$$
with
\begin{align*}
\boldsymbol \mu_{r|d} &=\mu_r \mathbf{i}_n + \sigma^2_r \boldsymbol \Sigma_r^\rho \mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}[\mathbf{d}-\mu_r\mathbf{H}\mathbf{i}_n]\\
\boldsymbol \Sigma_{r|d} &= \boldsymbol \Sigma_{r|d}^\sigma \boldsymbol \Sigma_{r|d}^\rho \boldsymbol \Sigma_{r|d}^\sigma = \sigma_r^2 \boldsymbol \Sigma_r^\rho - \sigma_r^2 \boldsymbol \Sigma_r^\rho\mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho
\end{align*}

We use as prior model one of the realizations with $\sigma_r^2 = 5$, $\nu = 1$ and Matern correlation function. Then, a prediction of the spatial variable $\{\hat{r}(\mathbf{x}); \mathbf{x}\in L\}$ represented by the vector $\hat{\mathbf{r}}$ is taken to minimize squared error, yielding
\begin{equation}
\hat{\mathbf{r}} = \mathbf{E}[\mathbf{r}|\mathbf{d}] = \boldsymbol \mu_{r|d}.
\label{rhat}
\end{equation}
The associated $(1-\alpha)$ prediction intervals are,
$$PI_\alpha = \boldsymbol \mu_{r|d} \pm z_{\alpha/2} \boldsymbol \sigma_{r|d},$$
where $\boldsymbol \sigma_{r|d}$ is a $n$-vector containing the diagonal elements of the standard deviation matrix $\boldsymbol \Sigma_{r|d}^\sigma$.

We compute and plot both the prediction and the interval.

```{r, echo = F, eval = T, out.width = '50%'}
sample = sample2[2,]
sigma_r_rho = sigmamatrix2
observation = c(sample[10], sample[25], sample[30])
H = matrix(0, nrow = 3, ncol = 50)
H[1,10] = 1
H[2,25] = 1
H[3,30] = 1
sigma_e = 0
sigma_dr = diag(3)*sigma_e
mu_rd_zero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_zero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_zero = sqrt(diag(sigma_rd_zero))
std_rd_zero[30] = 0
std_rd_zero[25] = 0
std_rd_zero[10] = 0
lower_bound_zero = mu_rd_zero - qnorm(0.95)*std_rd_zero
upper_bound_zero = mu_rd_zero + qnorm(0.95)*std_rd_zero


df = data.frame(y1 = mu_rd_zero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_zero, y4 = upper_bound_zero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0"))+
  theme(legend.position = 'none')

sigma_e = 0.25
sigma_dr = diag(3)*sigma_e
mu_rd_nonzero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_nonzero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_nonzero = sqrt(diag(sigma_rd_nonzero))
lower_bound_nonzero = mu_rd_nonzero - qnorm(0.95)*std_rd_nonzero
upper_bound_nonzero = mu_rd_nonzero + qnorm(0.95)*std_rd_nonzero

df = data.frame(y1 = mu_rd_nonzero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_nonzero, y4 = upper_bound_nonzero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0.25"))+
  theme(legend.title=element_blank())
```

From the plots, we see that with no observation error, the prediction coincides with the observations in the points where we have an observation. Also, in these points there is no uncertainty, so the lower and upper bounds are also equal the observations. When observation error is present, we get uncertainty in the observed points. In addition, adding an observation error leads to larger uncertainty, and thus variance, over the entire prediction.

## e)
We now go on to simulate 100 realizations from the posterior distribution, using the mean and covariance matrix previously computed. Then, we make a prediction based on the 100 samples and create a 90% empirical prediction interval by ordering the realizations and plotting the 5th smallest and largest value for each $x\in L$.

```{r, echo = F, eval = T,out.width = "50%"}
sample_zero = mvrnorm(100, mu_rd_zero, sigma_rd_zero)
prediction_zero = apply(sample_zero,2,mean)
std_empirical_zero = sqrt(apply(sample_zero,2,var))
lower_zero = prediction_zero - qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)
upper = prediction_zero + qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)

rownames(sample_zero) = paste("trial", seq(100), sep="")
colnames(sample_zero) = paste("x", seq(50), sep="")

pred_interval_zero <- apply(sample_zero,2,sort)

dat_zero = as.data.frame(sample_zero)
dat_zero$trial = rownames(dat_zero)
mdat_zero = melt(dat_zero, id.vars="trial")
mdat_zero$x = as.numeric(gsub("x", "", mdat_zero$variable))
#df_zero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_zero, ub = upper_zero)
df_zero = data.frame(x = seq(1,50), pred = prediction_zero, lb = pred_interval_zero[5,], ub = pred_interval_zero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_zero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_zero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_zero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_zero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0"))

sample_nonzero = mvrnorm(100, mu_rd_nonzero, sigma_rd_nonzero)
prediction_nonzero = apply(sample_nonzero,2,mean)
std_empirical_nonzero = sqrt(apply(sample_nonzero,2,var))
lower_nonzero = prediction_nonzero - qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)
upper = prediction_nonzero + qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)

rownames(sample_nonzero) = paste("trial", seq(100), sep="")
colnames(sample_nonzero) = paste("x", seq(50), sep="")

pred_interval_nonzero <- apply(sample_nonzero,2,sort)

dat_nonzero = as.data.frame(sample_nonzero)
dat_nonzero$trial = rownames(dat_nonzero)
mdat_nonzero = melt(dat_nonzero, id.vars="trial")
mdat_nonzero$x = as.numeric(gsub("x", "", mdat_nonzero$variable))
#df_nonzero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_nonzero, ub = upper_nonzero)
df_nonzero = data.frame(x = seq(1,50), pred = prediction_nonzero, lb = pred_interval_nonzero[5,], ub = pred_interval_nonzero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_nonzero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_nonzero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_nonzero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_nonzero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0.25"))
```
From the plots, we again see that with no observation error, the prediction coincides with the observations in the points where we have an observation, and we get zero prediction variance. However, because we here only have simulated values (and only 100 simulations), the variance with observation error is not larger for all other points than the variance without observation error.

Also, we see that the variance and prediction based on the 100 samples coincides well with the theoretical prediction and variance. Still, we observe lower degree of smoothness in the lines, which is as expected when comparing simulated values with theoretical quantities.
## f)
We use the previously generated $n=100$ realizations $\{r_i(x)\}, i \in 1,\dots,n$ with $\sigma_\epsilon^2 = 0$ to provide a prediction $\hat{A}_r$ for the non-linear function on $\{r(x);x\in D\}$,
$$A_r = \int_D I(r(x)>2)dx.$$
The predictor $\hat{A}_r$ will be
$$\hat{A}_r = \frac{1}{n} \sum_{i=1}^n A_{r_i}.$$
We also compute the prediction variance, which will be the sample variance, given as
$$ \text{Var}(\hat{A}_r) = \frac{1}{n-1}\sum_{i=1}^n (A_{r_i}-\hat{A}_r)^2.$$
An alternative predictor is,
$$\tilde{A}_r = \sum_{x\in L} I(\hat{r}(x)>2),$$
with $\hat{r}(x)$ given as in equation \eqref{rhat}. This value is also computed.

```{r, echo = F, eval = T}
extract <- function(sample){
  return(length(sample[which(sample>2)]))
}
A_hat = apply(sample_zero, 1,extract)
cat("A hat: ", mean(A_hat), '\n')

sigma_r_rho = sigmamatrix2
observations = matrix(c(sample2[,10], sample2[,25], sample2[,30]), nrow = 10, ncol = 3)
sigma_e = 0
sigma_dr = diag(3)*sigma_e
r_hat = 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%t(observations)
A_tilde = apply(r_hat,2,extract)

cat("A tilde: ", mean(A_tilde), '\n')
cat("Predicted variance of A hat: ", var(A_hat))
```
From the print-out, we see that $\hat{A}_r > \tilde{A}_r$. To shed some light on this result, we make use of Jensen's inequality, which states that for a random variable $X$ and a convex function $\psi$,
$$\psi(E[X]) \leq E[\psi(X)].$$
We then note that $\hat{r}(x) = E[r_i(x)]$, and with $\psi(\xi) = I(\xi >2)$, we get after integrating over $D$ (summing over $L$),
$$\tilde{A}_r = \sum_{x \in L} I(\hat{r}(x)>2)\leq E\Big[\int_D I(r_i(x)>2)dx\Big] = \hat{A}_r.$$
## g)
We now want to sum up our experiences made during this exercise. Firstly, we have investigated and achieved greater understanding as to why correlation functions need to be positive definite. Furhtermore, the choice of correlation function and correlation function parameters has great implications for the structure of the Gaussian RF. By choosing a suitable correlation function, we can impact both smoothness and overall variance of the resulting RF. Including an observation error on the observations also has great impact on the prediction variance in those points, and smaller impact on points far away from the observations. We have also gotten a greater understanding in making predictions on Gaussian RFs, and experienced an application of Jensen's inequality.

# Problem 2: Gaussian RF - real data

## a)
```{r, echo = F, eval = T, out.width = "50%"}
data = read.table("https://www.math.ntnu.no/emner/TMA4250/2017v/Exercise1/topo.dat")
interpolation <- interp(data$x, data$y, data$z)
contour(interpolation, drawlabels = F)
image.plot(interpolation)

#x = rep(interpolation$x,length(interpolation$x))
#y = rep(interpolation$y,each = length(interpolation$y))
#mtrx3d <- data.frame(x = x, y = y, z = as.vector(interpolation$z))
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))
```

A stationary Gaussian RF is a reasonable model for the terrain elevation in domain D provided the correlation function is such that the dependence between different points is decreasing with distance. This RF is ergodic as well, which is natural for the model. 

## b)
We now want to develop the expression for the minimization problem to be solved for
the universal kriging predictor. We assume the model parameters $\boldsymbol \beta_r^+$ to be unknown, while the parameters ($\sigma_r^2,\eta_r$) are known. Then, we need the kriging predictor $\hat r_0$ to be unbiased, which requires
$$\mathbf{E}[\hat r_0 - r_0] = \boldsymbol \alpha^T\mathbf{E}[\mathbf{r}^d] - \mathbf{E}[r_0] = 0,$$
implying
$$\boldsymbol \alpha^T\mathbf{G}_d\boldsymbol \beta_r^+ = \mathbf{g}_\mathbf{x_0}\boldsymbol \beta_r^+,$$
which finally implies
$$\mathbf{G}_d^T \boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.$$
In addition to being unbiased, we want the predictor to have minimum variance. We ensure minimum variance through the minimum squared-error criterion, which leads to the following minimization problem
\begin{align*}
\hat{\boldsymbol \alpha} &= \arg \min_{\boldsymbol \alpha} \{\mathbf{Var}[\hat r_0 - r_0]\}\\
&= \arg \min_{\boldsymbol \alpha} \{\sigma_r^2 - 2 \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \rho_0 + \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \Sigma_d^\rho \boldsymbol \alpha\}\\
&\text{constrained by: } \mathbf{G}_d^T\boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.
\end{align*}
This is a quadratic optimization problem with linear constraints. Thus, it is possible to solve it analytically, giving the universal Kriging predictor and associated prediction variance,
\begin{align*}
\hat r_0&=\hat{\boldsymbol \alpha}^T\mathbf{r}^d\\
\sigma_{\hat r}^2 &= \sigma_r^2[1-2\hat{\boldsymbol \alpha}^T\boldsymbol \rho_0 + \hat{\boldsymbol \alpha}^T\boldsymbol \Sigma_d^\rho \hat{\boldsymbol \alpha}],
\end{align*}
with 
$$\hat{\boldsymbol \alpha} = [\boldsymbol \Sigma_d^\rho]^{-1} \Big[\boldsymbol \rho_0 - \mathbf{G}_d^T\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\mathbf{G}_d^T\big]^{-1}\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\boldsymbol \rho_0 - \mathbf{g}_\mathbf{x_0}\big]\Big].$$
## c)
We now let the reference variable $\mathbf{x} \in D \subset \mathbb{R}^2$ be denoted $\mathbf{x} = (x_v,x_h)$ and set $n_g = 6$. Then we define the set of known polynomial functions $\mathbf{g}(\mathbf{x})$ to be all polynomial $x_v^k x_h^l$ for $(k,l) \in \{(0,0),(1,0),(0,1),(1,1),(2,0),(0,2)\}$. The resulting $n_g$-vector $\mathbf{g}(\mathbf{x})$ will be
$$\mathbf{g}(\mathbf{x}) = \{1,x_v, x_h, x_vx_h, x_v^2, x_h^2\}.$$
This gives the following expectation value for $r(\mathbf{x})$
$$\mathbf{E}(r(\mathbf{x})) = \mathbf{g}(\mathbf{x})^T\boldsymbol \beta_r = \beta_1 + x_v \beta_2 + x_h \beta_3 + x_v x_h \beta_4 + x_v^2 \beta_5 + x_h^2 \beta_6.$$

We can interpret the model such that mean is the level around which the elevation fluctuates, while the variance is the degree of fluctuation. The level is here a second order polynomial in the coordinates, while the variance is constant over the coordinates. 

```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# 2nd
kriging_2nd <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd$predict, nrow = 315))

# 1st
kriging_1st <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st$predict, nrow = 315))

```
-Is it natural to change the value of the variance if the parametrization of expectation function is changed? Present the results and comment on them, and the relation to the results using a higher-order expectation function.

If we change the parametrization of the expectation function, it is natural the change the value of the variance parameter. For example when we reduce the degree of the polynomial in the expectation function we might have to compensate with larger variance to get bigger fluctutations around this new mean level. 

The kriging predictions looks very similar (second degree polynomial to the left, and first degree to the right), but there are some differences, particularly in the corners of the field, which might be due to the difference in order of the expectation function.

## d)
Since the random field is Gaussian, the best linear predictor (our kriging predictor) coencides with the conditional expectation, $E(r_0 | \mathbf{r^d})$. The Gaussian distribution is closed under conditioning, and it follows that $$r_0 | \mathbf{r^d} \sim \phi(r_0; \hat{r}, \sigma^2_{\hat{r}}),$$ where $\sigma^2_{\hat{r}}$ is as previously defined. 

```{r, echo = F, eval = T}
# index of (100, 100)
idx = 100 + 100 * 315
mu = kriging_2nd$predict[idx]
var = kriging_2nd$krige.var[idx]

cat("Probability of r((100, 100)) being larger than 700m: ", pnorm((700 - mu) / sqrt(var), lower.tail = FALSE), '\n')

cat("Elevation at which the probability of r((100, 100)) being smaller is 0.9: ", qnorm(0.9, mean = mu, sd = sqrt(var), lower.tail = TRUE))

```

## e)

```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# nugget = 5

# 2nd
kriging_2nd_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_5$predict, nrow = 315))

# 1st
kriging_1st_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_5$predict, nrow = 315))

# nugget = 25

# 2nd
kriging_2nd_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_25$predict, nrow = 315))

# 1st
kriging_1st_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_25$predict, nrow = 315))

```

The result is plotted above. The plots to the left have second order expectation functions while the ones to the right have first order. The plots at the top have measurement variance $5$, while the plots at the bottom have measurement variance $25$. They all seem very similar, but there seems to be a much bigger difference going from left to right than from top to bottom. 

## f)

Our experiences using real data in this exercise has been useful, and it has been helpful to visualize the data in order to get a better feeling with the models and methods used. But it could be useful to get introduced to some of the algorithms used for making the predictions, instead of just putting the data in a black box. 

# Problem 3: Parameter estimation
We consider the stationary Gaussian RF $\{r(\mathbf{x})$; $\mathbf{x} \in D \subset \mathbb{R}^2\}$ with $D: [(1,30), (1,30)]$, with 
\begin{align*}
E\{r(\mathbf{x})\} &= \mu_r = 0\\
Var\{r(\mathbf{x})\} &= \sigma_r^2\\
Coor\{r(\mathbf{x}), r(\mathbf{x}')\} &= \exp \{-\tau/\xi_r\},
\end{align*}
with $\tau = |\mathbf{x} - \mathbf{x}'|$.

## a)
We discretize the Gaussian RF to get $\{r(x); x \in L\}$ on a grid $L : [30\times 30] \in D$. The model parameters are set to $\sigma_r^2 = 2$ and $\xi_r = 15$. Then we generate one realization of the discretized Gaussian RF and display it.

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
x = rep(seq(1, 30, length.out = 30),30)
y = rep(seq(1, 30, length.out = 30),each = 30)
dat = matrix(c(x,y), nrow = 900, ncol = 2)
distances = dist(dat)
distances <- as.matrix(distances)
sigma_r = 2
xi_r = 15
corr <- function(x, xi){
  return(exp(-x/xi))
}
corrmatrix = corr(distances, xi_r)
covmatrix = sigma_r*corrmatrix
mu_r = rep(0,900)
set.seed(1234567)
sample = mvrnorm(1,mu_r,covmatrix)

#mtrx3d <- data.frame(x = x, y = y, z = sample)
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))

x = seq(1,30)
y = seq(1,30)

image.plot(x = x, y = y, z = matrix(sample, ncol = 30))
```
From the display, it is reasonable to assume that the generated realization comes from a Gaussian RF. 

## b)
We now compute the empirical variogram based on the exact observations of the full realization displayed above. Then, the estimated variogram is displayed along with the correct variogram function $\gamma(\tau) = 1 - \rho(\tau)$.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
vario <- variog(messages = F, coords = dat, data = sample)
df <- data.frame(x = seq(0,40))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario$v, u = vario$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram")
```
From the plot, we see that the empirical variogram differs somewhat from the theoretical variogram. The two variograms coincide well for small values of $\tau$, but with greater $\tau$ the variograms differ significantly. This is as to be expected, because the empirical variogram is generated on relatively few observations and only one realization.


## c)
We now generate 36 locations uniformly randomly on the grid $L$. The we compute the empirical variogram estimate based on the corresponding 36 exact observations. The estimate is then displayed jointly with the theoretical variogram function.

Then we consider the model parameters $\sigma_r^2$ and $\xi_r$ to be unknown. These are then estimated by a maximum likelihood criterion based on exact observation of the full realization and based on the 36 observations. The two estimated variogram functions are then jointly displayed with the correct variogram function.
```{r, echo = F, eval = T, out.width = "50%"}
n_obs = 36
set.seed(12)
obs36 <- sample(1:900)[1:n_obs]
x_loc36 <- obs36%%30
y_loc36 <- obs36%/%30+1
coordinates = matrix(c(x_loc36,y_loc36), nrow = n_obs, ncol = 2)

datapoints = sample[obs36]
vario2 <- variog(messages = F,coords = coordinates, data = datapoints)
df <- data.frame(x = seq(0,max(as.matrix(dist(coordinates)))))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario2$v, u = vario2$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram")


parameters <- likfit(messages = F, coords = dat, data = sample, ini.cov.pars = c(1,10), cov.model = "exponential")
parameters2 <- likfit(messages = F, coords = coordinates, data = datapoints, ini.cov.pars = c(1,10), cov.model = "exponential")

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters2$cov.pars[1]*(1-corr(df$x,parameters2$cov.pars[2]))
ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial36")) +
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")
```
From the plots, we see that the empirical variogram based on only 36 observations fits the correct variogram worse than the one based on all 900 observations. However, the estimated variogram based on the 36 observations fits the correct variogram better than the estimated variogram based on the full realization. This result seems spurious, as adding more observations should in theory provide a better estimate of the true parameters.


## d)

```{r, echo = F, eval = T, out.width = "33%"}
n_obs = 9
obs9 <- sample(1:900)[1:n_obs]
x_loc9 <- obs9%%30
y_loc9 <- obs9%/%30+1
coordinates9 = matrix(c(x_loc9,y_loc9), nrow = n_obs, ncol = 2)
datapoints9 = sample[obs9]

parameters9 <- likfit(messages = F, coords = coordinates9, data = datapoints9, ini.cov.pars = c(1,10), cov.model = "exponential")
vario9 <- variog(messages = F, coords = coordinates9, data = datapoints9)
df2 <- data.frame(v = vario9$v, u = vario9$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters9$cov.pars[1]*(1-corr(df$x,parameters9$cov.pars[2]))
v1<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial9")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")

n_obs = 64
obs64 <- sample(1:900)[1:n_obs] 
x_loc64 <- obs64%%30
y_loc64 <- obs64%/%30+1
coordinates64 = matrix(c(x_loc64,y_loc64), nrow = n_obs, ncol = 2)
datapoints64 = sample[obs64]


parameters64 <- likfit(messages = F,coords = coordinates64, data = datapoints64, ini.cov.pars = c(1,10), cov.model = "exponential")
vario64 <- variog(messages = F, coords = coordinates64, data = datapoints64)
df2 <- data.frame(v = vario64$v, u = vario64$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters64$cov.pars[1]*(1-corr(df$x,parameters64$cov.pars[2]))
v2<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial64")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")

n_obs = 100
obs100 <- sample(1:900)[1:n_obs]
x_loc100 <- obs100%%30
y_loc100 <- obs100%/%30+1
coordinates100 = matrix(c(x_loc100,y_loc100), nrow = n_obs, ncol = 2)
datapoints100 = sample[obs100]

parameters100 <- likfit(messages = F,coords = coordinates100, data = datapoints100, ini.cov.pars = c(1,10), cov.model = "exponential")
vario100 <- variog(messages = F, coords = coordinates100, data = datapoints100)
df2 <- data.frame(v = vario100$v, u = vario100$u)

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters100$cov.pars[1]*(1-corr(df$x,parameters100$cov.pars[2]))
v3<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial100")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")
v1
v2
v3
```



