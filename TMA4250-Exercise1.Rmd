--- 
title: "TMA4250 Spatial Statistics Exercise 1, Spring 2019"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  word_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: no
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Ã˜yvind Auestad'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(reshape2)
library(geoR)
library(ggplot2)
library(MASS)
library(cowplot)
library(fields)
library(akima)

```

# Problem 1: Gaussian RF - model characteristics
We consider the continuous spatial variable $\{r(x):x\in D : [1,50] \subset \mathbb{R}^1$, and assume that it is modeled as a stationary 1D Gaussian RF with the following model parameters:
\begin{gather*}
\text{E}\{r(x)\} = \mu_r = 0\\
\text{Var}\{r(x)\} = \sigma_r^2\\
\text{Corr}\{r(x), r(x')\} = \rho_r(\tau),
\end{gather*}
where $\rho_r(\tau)$; $\tau = \rvert x-x'\lvert/10$ is the spatial correlation function. Let $D:[1,50]$
be discretised in $L \in \{1, 2,\dots , 50\}$ and define the discretised Gaussian RF
$\{r(x); x \in L\}$, represented by the $n$-vector $\mathbf{r}\in\mathbb{R}^n$. 


Let the spatial correlation function $\rho_r(\tau)$, be either Powered exponential with parameter $\nu_r \in [1, 1.9]$ or Matern with parameter $\nu_r \in [1, 3]$. Let the variance take the values $\sigma_r^2 \in [1, 5]$.

## a)
We restrict the spatial correlation functions to be only positive definite functions. Covariance matrices need to be positive definite, and a positive definite correlation function ensures that all covariances matrices in our stationary Gaussian RF are positive definite for all configurations and dimensions. Further, we define a function $\rho(\tau):\mathbb{R}^q\rightarrow \mathbb{R}$ to be positive definite if
\begin{align*}
&\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j \rho(\mathbf{x}_i-\mathbf{x}_j) \geq 0,\\
&\text{for all configuations } [\mathbf{x}_1, \mathbf{x}_2,\ldots,\mathbf{x}_n]\in\mathbb{R}^{q\times n},\\
&\text{for all weights } \boldsymbol{\alpha} = (\alpha_1, \alpha_2,\ldots,\alpha_n)\in \mathbb{R}^n,\\
&\text{for all }n\in \mathbb{N}_+ \setminus \{1\}.
\end{align*}

To further investigate different correlation functions, we display two types of correlation functions, the Matern: $\rho(\tau) = 2^{1-\nu}/\Gamma(\nu) \cdot \tau^\nu \mathcal{B}_\nu(\tau)$, and Powered exponential: $\rho(\tau) = \exp(-\tau^\nu)$, for different values of $\nu_r$.

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
set.seed(123)
x = seq(0,5,length.out = 1001)
mat_1 = matern(x, 1, 1)
mat_2 = matern(x, 1, 3)
pexp_1 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1))
pexp_2 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1.9))
df = data.frame(y1 = mat_1, y2 = mat_2, y3 = pexp_1, y4 = pexp_2, value = x)
ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) +  
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(rho, "(", tau,")"))) + ggtitle(expression(paste("Display of two correlation functions for different values of ", nu)))
```

For all correlation functions $\rho_r(0)  =1$, and $\rho_r(\tau) \in [-1,1]$; $\tau\in\mathbb{R}_+$, because the function represents correlation between two random variables. The correlation functions are continuous everywhere, except at $\tau = 0$ where a step may occur. If the correlation function is continuous at $\tau = 0$, then the random field is continuous almost everywhere. Away from $\tau = 0$, the correlation function must be smooth. From the displayed correlation functions, we see that $\rho_r(\tau)\rightarrow 0$ as $\tau \rightarrow \infty$. This will be the case for all correlation functions, implying that two points $\mathbf{x}$ and $\mathbf{x}'$ will tend towards being uncorrelated as $\lvert\mathbf{x}-\mathbf{x}'\rvert\rightarrow\infty$. Uncorrelated Gaussian random fields means that they will be independent, and by extension asymptotics Gaussian random fields are ergodic.

The correlation functions define correlation between points in our random field. This means that larger values of the correlation function implies a smoother random field. From the plotted correlation function, we see that the function values increases for larger values of $\nu_r$. Consequently, larger values of $\nu_r$ implies smoother random fields for both these correlation functions.

We now define the variogram function, which for stationary Gaussian random fields is defined as $\gamma_r(\tau) = \sigma^2_r[1-\rho_r(\tau)]$. Then, we display the variogram functions associated with the previously plotted correlation functions for $\sigma_r^2 = 1$.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
gamma_1 = (1-mat_1)
gamma_2 = (1-mat_2)
gamma_3 = (1-pexp_1)
gamma_4 = (1-pexp_2)

df = data.frame(y1 = gamma_1, y2 = gamma_2, y3 = gamma_3, y4 = gamma_4, value = x)

ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) + 
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(gamma, "(", tau,")"))) + ggtitle(expression(paste("Variogram for two correlation functions for different values of ", nu)))+
  theme(legend.title=element_blank())
```

## b)
The prior Gaussion random field is defined on the discretized representation $L\in \{1,2,\dots,50\}$ by
$$\mathbf{r} \sim p(\mathbf{r}) = \phi_n(\mathbf{r};\mu_r \mathbf{i}_n, \sigma^2_r \boldsymbol{\Sigma}_r^\rho),$$
which is a discretized stationary Gaussian random field with expectation $\mu(\mathbf{x}) = \mu_r$, variance $\sigma^2(\mathbf{x}) = \sigma_r^2$ and correlation function $\rho(\mathbf{x},\mathbf{x}') = \rho_r(|\mathbf{x}-\mathbf{x}'|/10)$.

We now want to simulate ten realizations of the Gaussian random field on $L$ for all the previously displayed correlation functions and $\sigma_r^2 \in [1,5]$.

```{r, echo = F, eval = T}
make_plot <- function(df,string){
  p <-ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = X1, col = "x1")) + 
    geom_line(aes(y = X2, col = "x2")) +
    geom_line(aes(y = X3, col = "x3")) + 
    geom_line(aes(y = X4, col = "x4")) + 
    geom_line(aes(y = X5, col = "x5")) + 
    geom_line(aes(y = X6, col = "x6")) +
    geom_line(aes(y = X7, col = "x7")) + 
    geom_line(aes(y = X8, col = "x8")) +    
    geom_line(aes(y = X9, col = "x9")) + 
    geom_line(aes(y = X10, col = "x10"))+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=11))  +
    ggtitle(string)
  return(p)
}

x = seq(1,50)
tau = abs(outer(x,x,'-')/10)
mu_r = rep(0,50)

sigmamatrix1 = matern(tau,1,1)
sample1 = mvrnorm(10,mu_r,sigmamatrix1)
df = data.frame(x, t(sample1))
p1 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=1")))

sigmamatrix2 = 5*matern(tau,1,1)
sample2 = mvrnorm(10,mu_r,sigmamatrix2)
df = data.frame(x, t(sample2))
p2 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=5")))

sigmamatrix3 = matern(tau,1,3)
sample3 = mvrnorm(10,mu_r,sigmamatrix3)
df = data.frame(x, t(sample3))
p3 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=1")))

sigmamatrix4 = 5*matern(tau,1,3)
sample4 = mvrnorm(10,mu_r,sigmamatrix4)
df = data.frame(x, t(sample4))
p4 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=5")))

sigmamatrix5 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample5 = mvrnorm(10,mu_r,sigmamatrix5)
df = data.frame(x, t(sample5))
p5 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=1")))

sigmamatrix6 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample6 = mvrnorm(10,mu_r,sigmamatrix6)
df = data.frame(x, t(sample6))
p6 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=5")))

sigmamatrix7 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample7 = mvrnorm(10,mu_r,sigmamatrix7)
df = data.frame(x, t(sample7))
p7 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=1")))

sigmamatrix8 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample8 = mvrnorm(10,mu_r,sigmamatrix8)
df = data.frame(x, t(sample8))
p8 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=5")))


plot_grid(p1,p2, p3, p4, p5, p6, p7, p8,nrow = 2, ncol = 4)
```

From the display, we see that a greater value of $\nu_r$ leads to smoother random fields. Also, a higher value of $\sigma_r^2$ leads to larger variations around the expected level $\mu_r = 0$. Also, the Matern correlation function leads to smoother random fields than the Powered exponential function.


## c)
We now observe the spatial variable as $\{d(x);x\in[10,25,30]\subset L\}$ according to the acquisition model,
$$d(x) = r(x) + \epsilon(x) \quad \quad x\in[10,25,30],$$
with measurement errors $\epsilon(\cdot)$ centred, i.i.d Gaussian with variance $\sigma_\epsilon^2$. Further, we assume that $r(x)$ and $\epsilon(x')$ are independent for all $x, x'$.

The likelihood model $p(\mathbf{d}|\mathbf{r})$ links the observations to the spatial variable. The observations $\mathbf{d}$ are known, while $\mathbf{r}$ is the spatial variable. This means that the likelihood $p(\mathbf{d}|\mathbf{r})$ is not a pdf w.r.t. $\mathbf{r}$, and we do not need to normalize the likelihood.

We can define a Gauss-linear likelihood model relative to the discretized spatial variable $\mathbf{r}$, and observe $\mathbf{d}\in\mathbb{R}^m$ according to
$$[\mathbf{d}|\mathbf{r}] = \mathbf{H}\mathbf{r} + \boldsymbol \epsilon_{d|r} \sim p(\mathbf{d}|\mathbf{r}) = \phi_m(\mathbf{d};\mathbf{H}\mathbf{r},\boldsymbol \Sigma_{d|r}),$$
where $\mathbf{H}$ is a $(m\times n)$ observation matrix. In our case, because we have three observations, $m = 3$. Also, the observations are assumed to have i.i.d. errors, which implies $\boldsymbol \Sigma_{d|r} = \sigma_\epsilon^2 I_{m\times m}$.

## d)
The pdf for the discretised posterior Gaussian random field, given the observations is defined in the following way,
$$[\mathbf{r}|\mathbf{d}]\sim p(\mathbf{r}|\mathbf{d}) = \phi_n(\mathbf{r};\boldsymbol \mu_{r|d},\boldsymbol \Sigma_{r|d}),$$
with
\begin{align*}
\boldsymbol \mu_{r|d} &=\mu_r \mathbf{i}_n + \sigma^2_r \boldsymbol \Sigma_r^\rho \mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}[\mathbf{d}-\mu_r\mathbf{H}\mathbf{i}_n],\\
\boldsymbol \Sigma_{r|d} &= \boldsymbol \Sigma_{r|d}^\sigma \boldsymbol \Sigma_{r|d}^\rho \boldsymbol \Sigma_{r|d}^\sigma = \sigma_r^2 \boldsymbol \Sigma_r^\rho - \sigma_r^2 \boldsymbol \Sigma_r^\rho\mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho.
\end{align*}

We use as prior model one of the realizations with $\sigma_r^2 = 5$, $\nu = 1$ and Matern correlation function. Then, a prediction of the spatial variable $\{\hat{r}(\mathbf{x}); \mathbf{x}\in L\}$ represented by the vector $\hat{\mathbf{r}}$ is taken to minimize squared error, yielding
\begin{equation}
\hat{\mathbf{r}} = \mathbf{E}[\mathbf{r}|\mathbf{d}] = \boldsymbol \mu_{r|d}.
\label{rhat}
\end{equation}
The associated $(1-\alpha)$ prediction intervals are,
$$PI_\alpha = \boldsymbol \mu_{r|d} \pm z_{\alpha/2} \boldsymbol \sigma_{r|d},$$
where $\boldsymbol \sigma_{r|d}$ is a $n$-vector containing the diagonal elements of the standard deviation matrix $\boldsymbol \Sigma_{r|d}^\sigma$.

We compute and plot both the prediction and the interval.

```{r, echo = F, eval = T, out.width = '50%'}
sample = sample2[2,]
sigma_r_rho = sigmamatrix2
observation = c(sample[10], sample[25], sample[30])
H = matrix(0, nrow = 3, ncol = 50)
H[1,10] = 1
H[2,25] = 1
H[3,30] = 1
sigma_e = 0
sigma_dr = diag(3)*sigma_e
mu_rd_zero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_zero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_zero = sqrt(diag(sigma_rd_zero))
std_rd_zero[30] = 0
std_rd_zero[25] = 0
std_rd_zero[10] = 0
lower_bound_zero = mu_rd_zero - qnorm(0.95)*std_rd_zero
upper_bound_zero = mu_rd_zero + qnorm(0.95)*std_rd_zero


df = data.frame(y1 = mu_rd_zero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_zero, y4 = upper_bound_zero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0"))+
  theme(legend.position = 'none')

sigma_e = 0.25
sigma_dr = diag(3)*sigma_e
mu_rd_nonzero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_nonzero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_nonzero = sqrt(diag(sigma_rd_nonzero))
lower_bound_nonzero = mu_rd_nonzero - qnorm(0.95)*std_rd_nonzero
upper_bound_nonzero = mu_rd_nonzero + qnorm(0.95)*std_rd_nonzero

df = data.frame(y1 = mu_rd_nonzero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_nonzero, y4 = upper_bound_nonzero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0.25"))+
  theme(legend.title=element_blank())
```

From the plots, we see that with no observation error, the prediction coincides with the observations in the points where we have an observation. Also, in these points there is no uncertainty, so the lower and upper bounds are also equal the observations. When observation error is present, we get uncertainty in the observed points. In addition, adding an observation error leads to larger uncertainty, and thus variance, over the entire prediction.

## e)
We now go on to simulate 100 realizations from the posterior distribution, using the mean and covariance matrix previously computed. Then, we make a prediction based on the 100 samples and create a 90% empirical prediction interval by ordering the realizations and plotting the 5th smallest and largest value for each $x\in L$.

```{r, echo = F, eval = T,out.width = "50%"}
sample_zero = mvrnorm(100, mu_rd_zero, sigma_rd_zero)
prediction_zero = apply(sample_zero,2,mean)
std_empirical_zero = sqrt(apply(sample_zero,2,var))
lower_zero = prediction_zero - qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)
upper = prediction_zero + qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)

rownames(sample_zero) = paste("trial", seq(100), sep="")
colnames(sample_zero) = paste("x", seq(50), sep="")

pred_interval_zero <- apply(sample_zero,2,sort)

dat_zero = as.data.frame(sample_zero)
dat_zero$trial = rownames(dat_zero)
mdat_zero = melt(dat_zero, id.vars="trial")
mdat_zero$x = as.numeric(gsub("x", "", mdat_zero$variable))
#df_zero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_zero, ub = upper_zero)
df_zero = data.frame(x = seq(1,50), pred = prediction_zero, lb = pred_interval_zero[5,], ub = pred_interval_zero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_zero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_zero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_zero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_zero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0"))

sample_nonzero = mvrnorm(100, mu_rd_nonzero, sigma_rd_nonzero)
prediction_nonzero = apply(sample_nonzero,2,mean)
std_empirical_nonzero = sqrt(apply(sample_nonzero,2,var))
lower_nonzero = prediction_nonzero - qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)
upper = prediction_nonzero + qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)

rownames(sample_nonzero) = paste("trial", seq(100), sep="")
colnames(sample_nonzero) = paste("x", seq(50), sep="")

pred_interval_nonzero <- apply(sample_nonzero,2,sort)

dat_nonzero = as.data.frame(sample_nonzero)
dat_nonzero$trial = rownames(dat_nonzero)
mdat_nonzero = melt(dat_nonzero, id.vars="trial")
mdat_nonzero$x = as.numeric(gsub("x", "", mdat_nonzero$variable))
#df_nonzero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_nonzero, ub = upper_nonzero)
df_nonzero = data.frame(x = seq(1,50), pred = prediction_nonzero, lb = pred_interval_nonzero[5,], ub = pred_interval_nonzero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_nonzero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_nonzero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_nonzero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_nonzero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0.25"))
```
From the plots, we again see that with no observation error, the prediction coincides with the observations in the points where we have an observation, and we get zero prediction variance. However, because we here only have simulated values (and only 100 simulations), the variance with observation error is not larger for all other points than the variance without observation error.

Also, we see that the variance and prediction based on the 100 samples coincides well with the theoretical prediction and variance. Still, we observe lower degree of smoothness in the lines, which is as expected when comparing simulated values with theoretical quantities.

## f)

We use the previously generated $n=100$ realizations $\{r_i(x)\}, i \in 1,\dots,n$ with $\sigma_\epsilon^2 = 0$ to provide a prediction $\hat{A}_r$ for the non-linear function on $\{r(x);x\in D\}$,
$$A_r = \int_D I(r(x)>2)dx.$$
The predictor $\hat{A}_r$ will be
$$\hat{A}_r = \frac{1}{n} \sum_{i=1}^n A_{r_i}.$$
We also compute the prediction variance, which will be the sample variance, given as
$$ \text{Var}(\hat{A}_r) = \frac{1}{n-1}\sum_{i=1}^n (A_{r_i}-\hat{A}_r)^2.$$
An alternative predictor is,
$$\tilde{A}_r = \sum_{x\in L} I(\hat{r}(x)>2),$$
with $\hat{r}(x)$ given as in equation \eqref{rhat}. This value is also computed.

```{r, echo = F, eval = T}
extract <- function(sample){
  return(length(sample[which(sample>2)]))
}
A_hat = apply(sample_zero, 1,extract)
cat("A hat: ", mean(A_hat), '\n')

sigma_r_rho = sigmamatrix2
observations = matrix(c(sample2[,10], sample2[,25], sample2[,30]), nrow = 10, ncol = 3)
sigma_e = 0
sigma_dr = diag(3)*sigma_e
r_hat = 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%t(observations)
A_tilde = apply(r_hat,2,extract)

cat("A tilde: ", mean(A_tilde), '\n')
cat("Predicted variance of A hat: ", var(A_hat))
```
From the print-out, we see that $\hat{A}_r > \tilde{A}_r$. To shed some light on this result, we make use of Jensen's inequality, which states that for a random variable $X$ and a convex function $\psi$,
$$\psi(E[X]) \leq E[\psi(X)].$$
We then note that $\hat{r}(x) = E[r_i(x)]$, and with $\psi(\xi) = I(\xi >2)$, we get after integrating over $D$ (summing over $L$),
$$\sum_{x \in L} I(\hat{r}(x)>2)\leq \int_D E\big[I(r_i(x)>2)\big]dx.$$
Now, $\tilde{A}_r$ is equal to the left hand side, and $\hat{A}_r$ is an unbiased prediction of the right hand side, and one would therefore expect $\tilde{A}_r < \hat{A}_r$. 

## g)

We now want to sum up our experiences made during this exercise. Firstly, we have investigated and achieved greater understanding as to why correlation functions need to be positive definite. Furhtermore, the choice of correlation function and correlation function parameters has great implications for the structure of the Gaussian RF. By choosing a suitable correlation function, we can impact both smoothness and overall variance of the resulting RF. Including an observation error on the observations also has great impact on the prediction variance in those points, and smaller impact on points far away from the observations. We have also gotten a greater understanding in making predictions on Gaussian RFs, and experienced an application of Jensen's inequality.

# Problem 2: Gaussian RF - real data

In this problem, we consider observations of terrain elevation from a provided data file. The 52 data observations are located on a domain $D = (0,315)\times(0,315)\subset \mathbb{R}^2$. We let the 52-vector of exact observations be $\mathbf{d} = (r(\mathbf{x}_1^0),\dots,r(\mathbf{x}_{52}^0))^T$.

## a)
We then display the observations in two different ways.

```{r, echo = F, eval = T, out.width = "50%"}
data = read.table("https://www.math.ntnu.no/emner/TMA4250/2017v/Exercise1/topo.dat")
interpolation <- interp(data$x, data$y, data$z)
contour(interpolation, drawlabels = F)
image.plot(interpolation)

#x = rep(interpolation$x,length(interpolation$x))
#y = rep(interpolation$y,each = length(interpolation$y))
#mtrx3d <- data.frame(x = x, y = y, z = as.vector(interpolation$z))
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))
```

A stationary Gaussian RF is a reasonable model for the terrain elevation in domain D provided the correlation function is such that the dependence between different points is decreasing with distance. This RF is ergodic as well, which is natural for the model. 

## b)
We let the terrain elevation on the domain $D$ be modeled by the Gaussian RF $\{r(\mathbf{x});\mathbf{x}\in D \subset \mathbb{R}^2\}$ with
\begin{align*}
E\{r(\mathbf{x})\} & = \mathbf{g}(\mathbf{x})^T\boldsymbol \beta_r,\\
Var\{r(\mathbf{x})\}&=\sigma_r^2,\\
Corr\{r(\mathbf{x}),r(\mathbf{x}')\} &= \rho_r(\tau/\xi),
\end{align*}
where $\mathbf{g}(\mathbf{x}) = (g_1(\mathbf{x}),\dots,g_{n_g}(\mathbf{x}))^T$ is a $n_g$-vector of known explanatory variables on $\mathbf{x}\in D$, and $\boldsymbol \beta_r = (\beta_1,\dots,\beta_{n_g})^T$ is a $n_g$-vector of unknown parameters. Moreover, we let the variance $\sigma_r^2 = 2500$ and the spatial correlation function have $\xi = 100$ with $\tau = |\mathbf{x}-\mathbf{x}'|$.

We now want to develop the expression for the minimization problem to be solved for
the universal Kriging predictor. We assume the model parameters $\boldsymbol \beta_r^+$ to be unknown, while the parameters ($\sigma_r^2,\eta_r$) are known. Then, we need the Kriging predictor $\hat r_0$ to be unbiased, which requires
$$\mathbf{E}[\hat r_0 - r_0] = \boldsymbol \alpha^T\mathbf{E}[\mathbf{r}^d] - \mathbf{E}[r_0] = 0,$$
implying
$$\boldsymbol \alpha^T\mathbf{G}_d\boldsymbol \beta_r^+ = \mathbf{g}_\mathbf{x_0}\boldsymbol \beta_r^+,$$
which finally implies
$$\mathbf{G}_d^T \boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.$$
In addition to being unbiased, we want the predictor to have minimum variance. We ensure minimum variance through the minimum squared-error criterion, which leads to the following minimization problem
\begin{align*}
\hat{\boldsymbol \alpha} &= \arg \min_{\boldsymbol \alpha} \{\mathbf{Var}[\hat r_0 - r_0]\}\\
&= \arg \min_{\boldsymbol \alpha} \{\sigma_r^2 - 2 \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \rho_0 + \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \Sigma_d^\rho \boldsymbol \alpha\}\\
&\text{constrained by: } \mathbf{G}_d^T\boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.
\end{align*}
This is a quadratic optimization problem with linear constraints. Thus, it is possible to solve it analytically, giving the universal Kriging predictor and associated prediction variance,
\begin{align*}
\hat r_0&=\hat{\boldsymbol \alpha}^T\mathbf{r}^d\\
\sigma_{\hat r}^2 &= \sigma_r^2[1-2\hat{\boldsymbol \alpha}^T\boldsymbol \rho_0 + \hat{\boldsymbol \alpha}^T\boldsymbol \Sigma_d^\rho \hat{\boldsymbol \alpha}],
\end{align*}
with 
$$\hat{\boldsymbol \alpha} = [\boldsymbol \Sigma_d^\rho]^{-1} \Big[\boldsymbol \rho_0 - \mathbf{G}_d^T\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\mathbf{G}_d^T\big]^{-1}\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\boldsymbol \rho_0 - \mathbf{g}_\mathbf{x_0}\big]\Big].$$

## c)
We now let the reference variable $\mathbf{x} \in D \subset \mathbb{R}^2$ be denoted $\mathbf{x} = (x_v,x_h)$ and set $n_g = 6$. Then we define the set of known polynomial functions $\mathbf{g}(\mathbf{x})$ to be all polynomial $x_v^k x_h^l$ for $(k,l) \in \{(0,0),(1,0),(0,1),(1,1),(2,0),(0,2)\}$. The resulting $n_g$-vector $\mathbf{g}(\mathbf{x})$ will be
$$\mathbf{g}(\mathbf{x}) = \{1,x_v, x_h, x_vx_h, x_v^2, x_h^2\}.$$
This gives the following expectation value for $r(\mathbf{x})$
$$\mathbf{E}(r(\mathbf{x})) = \mathbf{g}(\mathbf{x})^T\boldsymbol \beta_r = \beta_1 + x_v \beta_2 + x_h \beta_3 + x_v x_h \beta_4 + x_v^2 \beta_5 + x_h^2 \beta_6.$$

We also present a model without the second order terms. 

We can interpret the model such that mean is the level around which the elevation fluctuates, while the variance is the degree of fluctuation. The level is here a second order polynomial in the coordinates, while the variance is constant over the coordinates. 

```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# 2nd
kriging_2nd <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

pred_brk = seq(650, 1000, l = 65)
var_brk = seq(0, 1700, l = 65)
color_table = tim.colors(64) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd$predict, nrow = 315), main = "Prediction. Second order mean function", breaks = pred_brk, col = color_table)

# 1st
kriging_1st <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st$predict, nrow = 315), main = "Prediction. First order mean function", breaks = pred_brk, col = color_table)

# variance

image.plot(x = x, y = y, z = matrix(kriging_2nd$krige.var, nrow = 315), main = "Variance. Second order mean function", breaks = var_brk, col = color_table)

image.plot(x = x, y = y, z = matrix(kriging_1st$krige.var, nrow = 315), main = "Variance. First order mean function", breaks = var_brk, col = color_table)

```

If we change the parametrization of the expectation function, it could be natural also to change the value of the model variance, $\sigma_r^2$. For example when we reduce the degree of the polynomial in the expectation function it could be that we might have to compensate with a larger model variance to get bigger fluctutations around this new 'less flexible' mean. But in practice it would be more natural to estimate the model variance using maximum likelihood, with our data points.

The Kriging predictions look very similar, but there are some differences, which become most clear in the corners of the field. The same is true for the Kriging variances. Adding more parameters in the expectation function, we get more constraints on the minimization problem for $\alpha$, and so a larger or equal Kriging variance. From the plots we see that the model with second order expectation has the largest Kriging variances, as expected.

From these plots it is clear where the observations are located. Moving further away from the observations we see that the Kriging variance increases. It always stays below the model variance ($2500$), with the maximum value being just above $1600$, obtained in the bottom left corner of the first variance plot. 

## d)

Since the random field is Gaussian, the best linear predictor (our kriging predictor) coincides with the conditional expectation, $E(r_0 | \mathbf{r^d})$. The Gaussian distribution is closed under conditioning, and it follows that
$$[r_0 | \mathbf{r^d}] \sim \phi(r_0; \hat{r}_0, \sigma^2_{\hat{r}}),$$
where $\sigma^2_{\hat{r}}$ is as previously defined. 

Then we get
$$P(r((100, 100)) < 700) = P\Big(\frac{r((100, 100)) - \hat{r}_0}{\sigma_{\hat{r}}} < \frac{700 - \hat{r}_0}{\sigma_{\hat{r}}}\Big) = \Phi \Big(\frac{700 - \hat{r}_0}{\sigma_{\hat{r}}}\Big),$$
and the elevation for which it is 0.9 probability that the true elevation is below it will be the solution, $r_{0.9}$, to
$$P(r((100, 100)) < r_{0.9}) = 0.9 \Rightarrow \Phi \Big(\frac{r_{0.9} - \hat{r}_0}{\sigma_{\hat{r}}}\Big) = 0.9.$$
```{r, echo = F, eval = T}
# index of (100, 100)
idx = 100 + 100 * 315
mu = kriging_2nd$predict[idx]
var = kriging_2nd$krige.var[idx]

cat("Probability of r((100, 100)) being larger than 700m: ", pnorm((700 - mu) / sqrt(var), lower.tail = FALSE), '\n')

cat("Elevation at which the probability of r((100, 100)) being smaller is 0.9: ", qnorm(0.9, mean = mu, sd = sqrt(var), lower.tail = TRUE))

```

## e)

We now assume that the observations in the $n_g-$vector $\mathbf{d}$ are associated with observation errors being centered Gaussian and independent from each other and the terrain elevation, with error variance $\sigma_{\epsilon}^2$. We calculate the Kriging predictions with associated prediction variance for the two values of the observation error variance, $\sigma_{\epsilon}^2 \in \{ 5, 25 \}$. The results are displayed below.

```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# nugget = 5

# 2nd
kriging_2nd_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_5$predict, nrow = 315), main = "Prediction. Second order mean function. \nMeasurement error variance = 5", breaks = pred_brk, col = color_table)

# 1st
kriging_1st_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_5$predict, nrow = 315), main = "Prediction. First order mean function. \nMeasurement error variance = 5", breaks = pred_brk, col = color_table)

# nugget = 25

# 2nd
kriging_2nd_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_25$predict, nrow = 315), main = "Prediction. Second order mean function. \nMeasurement error variance = 25", breaks = pred_brk, col = color_table)

# 1st
kriging_1st_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_25$predict, nrow = 315), main = "Prediction. First order mean function. \nMeasurement error variance = 25", breaks = pred_brk, col = color_table)


# variances 
image.plot(x = x, y = y, z = matrix(kriging_2nd_5$krige.var, nrow = 315), main = "Variance. Second order mean function. \nMeasurement error variance = 5", breaks = var_brk, col = color_table)

image.plot(x = x, y = y, z = matrix(kriging_1st_5$krige.var, nrow = 315), main = "Variance. First order mean function. \nMeasurement error variance = 5", breaks = var_brk, col = color_table)

image.plot(x = x, y = y, z = matrix(kriging_2nd_25$krige.var, nrow = 315), main = "Variance. Second order mean function. \nMeasurement error variance = 25", breaks = var_brk, col = color_table)

image.plot(x = x, y = y, z = matrix(kriging_1st_25$krige.var, nrow = 315), main = "Variance. First order mean function. \nMeasurement error variance = 25", breaks = var_brk, col = color_table)

```

It is hard to see a difference in the plots of the Kriging predictions. The same goes for the Kriging variances. The added measurement error is not large enough to be visible on the plot. But if we look at the Kriging variance values we see that a larger measurement error variance gives a larger Kriging variance, which we would expect. If we increase this variance even more we might be able to see the difference in the plots.

## f)

Our experiences using real data in this exercise has been useful. It has been helpful to visualize the data in order to get a better feeling with the models and methods used. But it could be useful to get introduced to some of the algorithms used for making the predictions, instead of just putting the data in a black box. 

# Problem 3: Parameter estimation
We consider the stationary Gaussian RF $\{r(\mathbf{x})$; $\mathbf{x} \in D \subset \mathbb{R}^2\}$ with $D: [(1,30), (1,30)]$, with 
\begin{align*}
E\{r(\mathbf{x})\} &= \mu_r = 0\\
Var\{r(\mathbf{x})\} &= \sigma_r^2\\
Coor\{r(\mathbf{x}), r(\mathbf{x}')\} &= \exp \{-\tau/\xi_r\},
\end{align*}
with $\tau = |\mathbf{x} - \mathbf{x}'|$.

## a)
We discretize the Gaussian RF to get $\{r(x); x \in L\}$ on a grid $L : [30\times 30] \in D$. The model parameters are set to $\sigma_r^2 = 2$ and $\xi_r = 15$. Then we generate one realization of the discretized Gaussian RF and display it.

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
x = rep(seq(1, 30, length.out = 30),30)
y = rep(seq(1, 30, length.out = 30),each = 30)
dat = matrix(c(x,y), nrow = 900, ncol = 2)
distances = dist(dat)
distances <- as.matrix(distances)
sigma_r = 2
xi_r = 15
corr <- function(x, xi){
  return(exp(-x/xi))
}
corrmatrix = corr(distances, xi_r)
covmatrix = sigma_r*corrmatrix
mu_r = rep(0,900)
set.seed(1234567)
sample = mvrnorm(1,mu_r,covmatrix)

#mtrx3d <- data.frame(x = x, y = y, z = sample)
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))

x = seq(1,30)
y = seq(1,30)

image.plot(x = x, y = y, z = matrix(sample, ncol = 30))
```
From the display, it is reasonable to assume that the generated realization comes from a Gaussian RF. This is as expected because the realization is generated from a Gaussian RF.

## b)
We now compute the empirical variogram based on the exact observations of the full realization displayed above. Then, the estimated variogram is displayed along with the correct variogram function $\gamma(\tau) = \sigma_r^2[1 - \rho(\tau)]$.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
vario <- variog(messages = F, coords = dat, data = sample)
df <- data.frame(x = seq(0,40))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario$v, u = vario$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram")
```
From the plot, we see that the empirical variogram differs somewhat from the theoretical variogram. The two variograms coincide well for small values of $\tau$, but with greater $\tau$ the variograms differ significantly. This is as to be expected, because the empirical variogram is generated on only one realization and relatively few observations, especialy for large values of $\tau$.


## c)
We now generate 36 locations uniformly randomly on the grid $L$. Then we compute the empirical variogram estimate based on the corresponding 36 exact observations. The estimate is then displayed jointly with the theoretical variogram function.

Then we consider the model parameters $\sigma_r^2$ and $\xi_r$ to be unknown. These are then estimated by a maximum likelihood criterion based on exact observation of the full realization and based on the 36 observations. The two estimated variogram functions are then jointly displayed with the correct variogram function.
```{r, echo = F, eval = T, out.width = "50%"}
n_obs = 36
set.seed(12)
obs36 <- sample(1:900)[1:n_obs]
x_loc36 <- obs36%%30
y_loc36 <- obs36%/%30+1
coordinates36 = matrix(c(x_loc36,y_loc36), nrow = n_obs, ncol = 2)

datapoints36 = sample[obs36]
vario2 <- variog(messages = F,coords = coordinates36, data = datapoints36)
df <- data.frame(x = seq(0,max(as.matrix(dist(coordinates36)))))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario2$v, u = vario2$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram with 36 observations") + theme(plot.title = element_text(size = 8))


parameters <- likfit(messages = F, coords = dat, data = sample, ini.cov.pars = c(1,10), cov.model = "exponential")
parameters36 <- likfit(messages = F, coords = coordinates36, data = datapoints36, ini.cov.pars = c(1,10), cov.model = "exponential")

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters36$cov.pars[1]*(1-corr(df$x,parameters36$cov.pars[2]))
ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full realization")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "36 observations")) +
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of estimated and theoretical variogram")
```
From the plots, we see that the empirical variogram based on only 36 observations fits the correct variogram worse than the one based on all 900 observations. However, the estimated variogram based on the 36 observations fits the correct variogram better than the estimated variogram based on the full realization. This result seems spurious, as adding more observations should in theory provide a better estimate of the true parameters.


## d)
We then repeat the process with 9, 64 and 100 exact observations from the realization, and present the results.
```{r, echo = F, eval = T, out.width = "33%"}
set.seed(1234)
n_obs = 9
obs9 <- sample(1:900)[1:n_obs]
x_loc9 <- obs9%%30
y_loc9 <- obs9%/%30+1
coordinates9 = matrix(c(x_loc9,y_loc9), nrow = n_obs, ncol = 2)
datapoints9 = sample[obs9]

parameters9 <- likfit(messages = F, coords = coordinates9, data = datapoints9, ini.cov.pars = c(1,10), cov.model = "exponential")
vario9 <- variog(messages = F, coords = coordinates9, data = datapoints9)
df2 <- data.frame(v = vario9$v, u = vario9$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters9$cov.pars[1]*(1-corr(df$x,parameters9$cov.pars[2]))
v1<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full realization")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "9 observations")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical 9 obs")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical, estimated and \ntheoretical variogram with 9 observations") + theme(plot.title = element_text(size = 10))

n_obs = 64
obs64 <- sample(1:900)[1:n_obs] 
x_loc64 <- obs64%%30
y_loc64 <- obs64%/%30+1
coordinates64 = matrix(c(x_loc64,y_loc64), nrow = n_obs, ncol = 2)
datapoints64 = sample[obs64]


parameters64 <- likfit(messages = F,coords = coordinates64, data = datapoints64, ini.cov.pars = c(1,10), cov.model = "exponential")
vario64 <- variog(messages = F, coords = coordinates64, data = datapoints64)
df2 <- data.frame(v = vario64$v, u = vario64$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters64$cov.pars[1]*(1-corr(df$x,parameters64$cov.pars[2]))
v2<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full realization")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "64 observations")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical 64 obs")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical, estimated and \ntheoretical variogram with 64 observations") + theme(plot.title = element_text(size = 10))

n_obs = 100
obs100 <- sample(1:900)[1:n_obs]
x_loc100 <- obs100%%30
y_loc100 <- obs100%/%30+1
coordinates100 = matrix(c(x_loc100,y_loc100), nrow = n_obs, ncol = 2)
datapoints100 = sample[obs100]

parameters100 <- likfit(messages = F,coords = coordinates100, data = datapoints100, ini.cov.pars = c(1,10), cov.model = "exponential")
vario100 <- variog(messages = F, coords = coordinates100, data = datapoints100)
df2 <- data.frame(v = vario100$v, u = vario100$u)

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters100$cov.pars[1]*(1-corr(df$x,parameters100$cov.pars[2]))
v3<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full realization")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "100 observations")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical 100 obs")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical, estimated and \ntheoretical variogram with 100 observations") + theme(plot.title = element_text(size = 10))
v1
v2
v3
```
From the plots, we see that including more observations gives a better fit to the correct variogram for both the empirical and the estimated variogram. 

## e)
In this problem, we have used the variogram function for the first time, and used maximum likelihood to find covariance parameters for the first time. The achieved results seemed bizarre, as 36 observations provided a better fit than 900 observations. We suspect this result to be due to chance, and would not expect to get the same result if the experiment was repeated.
