--- 
title: "TMA4250 Spatial Statistics Exercise 1, Spring 2019"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  word_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: no
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Ã˜yvind Auestad (SKRIVE STUDENTNUMMER
  I STEDET??)'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(reshape2)
library(geoR)
library(ggplot2)
library(MASS)
library(cowplot)
library(fields)
library(akima)

```

# Problem 1: Gaussian RF - model characteristics
We consider the continuous spatial variable $\{r(x):x\in D : [1,50] \subset \mathbb{R}^1$, and assume that it is modeled as a stationary 1D Gaussian RF with the following model parameters:
\begin{gather*}
\text{E}\{r(x)\} = \mu_r = 0\\
\text{Var}\{r(x)\} = \sigma_r^2\\
\text{Corr}\{r(x), r(x')\} = \rho_r(\tau),
\end{gather*}
where $\rho_r(\tau)$; $\tau = \rvert x-x'\lvert/10$ is the spatial correlation function. Let $D:[1,50]$
be discretised in $L \in \{1, 2,\dots , 50\}$ and define the discretised Gaussian RF
$\{r(x); x \in L\}$, represented by the $n$-vector $\mathbf{r}\in\mathbb{R}^n$. 


Let the spatial correlation function $\rho_r(\tau)$, be either Powered exponential with parameter $\nu_r \in [1, 1.9]$ or Matern with parameter $\nu_r \in [1, 3]$. Let the variance take the values $\sigma_r^2 \in [1, 5]$.

## a)
We restrict the spatial correlation functions to be only positive definite functions. Covariance matrices need to be positive definite, and a positive definite correlation function ensures that all covariances matrices in our stationary Gaussian RF are positive definite for all configurations and dimensions. Further, we define a function $\rho(\tau):\mathbb{R}^q\rightarrow \mathbb{R}$ to be positive definite if
\begin{align*}
&\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j \rho(\mathbf{x}_i-\mathbf{x}_j) \geq 0,\\
&\text{for all configuations } [\mathbf{x}_1, \mathbf{x}_2,\ldots,\mathbf{x}_n]\in\mathbb{R}^{q\times n},\\
&\text{for all weights } \boldsymbol{\alpha} = (\alpha_1, \alpha_2,\ldots,\alpha_n)\in \mathbb{R}^n,\\
&\text{for all }n\in \mathbb{N}_+ \setminus \{1\}.
\end{align*}

To further investigate different correlation functions, we display two types of correlation functions, the Matern and Powered exponential functions, for different values of $\nu_r$. 

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
set.seed(123)
x = seq(0,5,length.out = 1001)
mat_1 = matern(x, 1, 1)
mat_2 = matern(x, 1, 3)
pexp_1 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1))
pexp_2 = cov.spatial(x, cov.model = "exponential", cov.pars = c(1, 1.9))
df = data.frame(y1 = mat_1, y2 = mat_2, y3 = pexp_1, y4 = pexp_2, value = x)
ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) +  
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(rho, "(", tau,")"))) + ggtitle(expression(paste("Display of two correlation functions for different values of ", nu)))
```

For all correlation functions $\rho_r(0)  =1$, and $\rho_r(\tau) \in [-1,1]$; $\tau\in\mathbb{R}_+$, because the function represents correlation between two random variables. The correlation functions are continuous everywhere, except at $\tau = 0$ where a step may occur. If the correlation function is continuous at $\tau = 0$, then the random field is continuous almost everywhere. Away from $\tau = 0$, the correlation function must be smooth. From the displayed correlation functions, we see that $\rho_r(\tau)\rightarrow 0$ as $\tau \rightarrow \infty$. This will be the case for all correlation functions, implying that two points $\mathbf{x}$ and $\mathbf{x}'$ will tend towards being uncorrelated as $\lvert\mathbf{x}-\mathbf{x}'\rvert\rightarrow\infty$. Uncorrelated Gaussian random fields means that they will be independent, and by extension asymptotics Gaussion random fields are ergodic.

The correlation functions define correlation between points in our random field. This means that larger values of the correlation function implies a smoother random field. From the plotted correlation function, we see that the function values increases for larger values of $\nu_r$. Consequently, larger values of $\nu_r$ implies smoother random fields for both these correlation functions.

We now define the variogram function, which for stationary Gaussian random fields is defined as $\gamma_r(\tau) = \sigma^2_r[1-\rho_r(\tau)]$. Then, we display the variogram functions associated with the previously plotted correlation functions for $\sigma_r^2 = 1$.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
gamma_1 = (1-mat_1)
gamma_2 = (1-mat_2)
gamma_3 = (1-pexp_1)
gamma_4 = (1-pexp_2)

df = data.frame(y1 = gamma_1, y2 = gamma_2, y3 = gamma_3, y4 = gamma_4, value = x)

ggplot(df, aes(x, y = value, color = variable)) + 
  geom_line(aes(y = y1, colour = "y1")) + 
  geom_line(aes(y = y2, colour = "y2")) +
  geom_line(aes(y = y3, colour = "y3")) + 
  geom_line(aes(y = y4, colour = "y4")) + 
  scale_color_manual(labels = c(expression(paste("Matern ",nu, "=1")), expression(paste("Matern ",nu, "=3")),expression(paste("PExp ", nu, "=1")), expression(paste("PExp ", nu, "=1.9"))), values = c("red", "blue", "green", "purple")) + xlab(expression(paste(tau))) + ylab(expression(paste(gamma, "(", tau,")"))) + ggtitle(expression(paste("Variogram for two correlation functions for different values of ", nu)))+
  theme(legend.title=element_blank())
```

## b)
The prior Gaussion random field is defined on the discretized representation $L\in \{1,2,\dots,50\}$ by
$$\mathbf{r} \sim p(\mathbf{r}) = \phi_n(\mathbf{r};\mu_r \mathbf{i}_n, \sigma^2_r \boldsymbol{\Sigma}_r^\rho),$$
which is a discretized stationary Gaussian random field with expectation $\mu(\mathbf{x}) = \mu_r$, variance $\sigma^2(\mathbf{x}) = \sigma_r^2$ and correlation function $\rho(\mathbf{x},\mathbf{x}') = \rho_r(\mathbf{x}-\mathbf{x}')$.

We now want to simulate ten realizations of the Gaussian random field on $L$ for all the previously displayed correlation functions and $\sigma_r^2 \in [1,5]$.

```{r, echo = F, eval = T}
make_plot <- function(df,string){
  p <-ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = X1, col = "x1")) + 
    geom_line(aes(y = X2, col = "x2")) +
    geom_line(aes(y = X3, col = "x3")) + 
    geom_line(aes(y = X4, col = "x4")) + 
    geom_line(aes(y = X5, col = "x5")) + 
    geom_line(aes(y = X6, col = "x6")) +
    geom_line(aes(y = X7, col = "x7")) + 
    geom_line(aes(y = X8, col = "x8")) +    
    geom_line(aes(y = X9, col = "x9")) + 
    geom_line(aes(y = X10, col = "x10"))+  theme(legend.position = "none",axis.title.x=element_blank(),
        axis.text.x=element_blank(),axis.title.y=element_blank(), plot.title = element_text(size=11))  +
    ggtitle(string)
  return(p)
}

x = seq(1,50)
tau = abs(outer(x,x,'-')/10)
mu_r = rep(0,50)

sigmamatrix1 = matern(tau,1,1)
sample1 = mvrnorm(10,mu_r,sigmamatrix1)
df = data.frame(x, t(sample1))
p1 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=1")))

sigmamatrix2 = 5*matern(tau,1,1)
sample2 = mvrnorm(10,mu_r,sigmamatrix2)
df = data.frame(x, t(sample2))
p2 <- make_plot(df, expression(paste("Matern, ", nu, "=1, ", sigma, "=5")))

sigmamatrix3 = matern(tau,1,3)
sample3 = mvrnorm(10,mu_r,sigmamatrix3)
df = data.frame(x, t(sample3))
p3 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=1")))

sigmamatrix4 = 5*matern(tau,1,3)
sample4 = mvrnorm(10,mu_r,sigmamatrix4)
df = data.frame(x, t(sample4))
p4 <- make_plot(df, expression(paste("Matern, ", nu, "=3, ", sigma, "=5")))

sigmamatrix5 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample5 = mvrnorm(10,mu_r,sigmamatrix5)
df = data.frame(x, t(sample5))
p5 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=1")))

sigmamatrix6 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1))
sample6 = mvrnorm(10,mu_r,sigmamatrix6)
df = data.frame(x, t(sample6))
p6 <- make_plot(df, expression(paste("PExp, ", nu, "=1, ", sigma, "=5")))

sigmamatrix7 = cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample7 = mvrnorm(10,mu_r,sigmamatrix7)
df = data.frame(x, t(sample7))
p7 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=1")))

sigmamatrix8 = 5*cov.spatial(tau, cov.model = "exponential", cov.pars = c(1, 1.9))
sample8 = mvrnorm(10,mu_r,sigmamatrix8)
df = data.frame(x, t(sample8))
p8 <- make_plot(df, expression(paste("PExp, ", nu, "=1.9, ", sigma, "=5")))


plot_grid(p1,p2, p3, p4, p5, p6, p7, p8,nrow = 2, ncol = 4)
```

From the display, we see that a greater value of $\nu_r$ leads to smoother random fields. Also, a higher value of $\sigma_r^2$ leads to larger variations around the expected level $\mu_r = 0$. Also, the Matern correlation function leads to smoother random fields than the Powered exponential function.


## c)
We now observe the spatial variable as $\{d(x);x\in[10,25,30]\subset L\}$ according to the acquisition model,
$$d(x) = r(x) + \epsilon(x) \quad \quad x\in[10,25,30],$$
with measurement errors $\epsilon(\cdot)$ centred, i.i.d Gaussian with variance $\sigma_\epsilon^2$. Further, we assume that $r(x)$ and $\epsilon(x')$ are independent for all $x, x'$.

The likelihood model $p(\mathbf{d}|\mathbf{r})$ links the observations to the spatial variable. The observations $\mathbf{d}$ are known, while $\mathbf{r}$ is the spatial variable. This means that the likelihood $p(\mathbf{d}|\mathbf{r})$ is not a pdf w.r.t. $\mathbf{r}$, and we do not need to normalize the likelihood.

We can define a Gauss-linear likelihood model relative to the discretized spatial variable $\mathbf{r}$, and observe $\mathbf{d}\in\mathbb{R}^m$ according to
$$[\mathbf{d}|\mathbf{r}] = \mathbf{H}\mathbf{r} + \boldsymbol \epsilon_{d|r} \sim p(\mathbf{d}|\mathbf{r}) = \phi_m(\mathbf{d};\mathbf{H}\mathbf{r},\boldsymbol \Sigma_{d|r}),$$
where $\mathbf{H}$ is a $(m\times n)$ observation matrix. In our case, because we have three observations, $m = 3$. Also, the observations are assumed to have i.i.d. errors, which implies $\boldsymbol \Sigma_{d|r} = \sigma_\epsilon^2 I_{m\times m}$.

## d)
The pdf for the discretised posterior Gaussian random field, given the observations is defined in the following way,
$$[\mathbf{r}|\mathbf{d}]\sim p(\mathbf{r}|\mathbf{d}) = \phi_n(\mathbf{r};\boldsymbol \mu_{r|d},\boldsymbol \Sigma_{r|d},)$$
with
\begin{align*}
\boldsymbol \mu_{r|d} &=\mu_r \mathbf{i}_n + \sigma^2_r \boldsymbol \Sigma_r^\rho \mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}[\mathbf{d}-\mu_r\mathbf{H}\mathbf{i}_n]\\
\boldsymbol \Sigma_{r|d} &= \boldsymbol \Sigma_{r|d}^\sigma \boldsymbol \Sigma_{r|d}^\rho \boldsymbol \Sigma_{r|d}^\sigma = \sigma_r^2 \boldsymbol \Sigma_r^\rho - \sigma_r^2 \boldsymbol \Sigma_r^\rho\mathbf{H}^T\big[\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho\mathbf{H} + \boldsymbol \Sigma_{d|r}\big]^{-1}\sigma_r^2\mathbf{H}\boldsymbol \Sigma_r^\rho
\end{align*}

We use as prior model one of the realizations with $\sigma_r^2 = 5$, $\nu = 1$ and Matern correlation function. Then, a prediction of the spatial variable $\{\hat{r}(\mathbf{x}); \mathbf{x}\in L\}$ represented by the vector $\hat{\mathbf{r}}$ is taken to minimize squared error, yielding
\begin{equation}
\hat{\mathbf{r}} = \mathbf{E}[\mathbf{r}|\mathbf{d}] = \boldsymbol \mu_{r|d}.
\label{rhat}
\end{equation}
The associated $(1-\alpha)$ prediction intervals are,
$$PI_\alpha = \boldsymbol \mu_{r|d} \pm z_{\alpha/2} \boldsymbol \sigma_{r|d},$$
where $\boldsymbol \sigma_{r|d}$ is a $n$-vector containing the diagonal elements of the standard deviation matrix $\boldsymbol \Sigma_{r|d}^\sigma$.

We compute and plot both the prediction and the interval.

```{r, echo = F, eval = T, out.width = '50%'}
sample = sample2[2,]
sigma_r_rho = sigmamatrix2
observation = c(sample[10], sample[25], sample[30])
H = matrix(0, nrow = 3, ncol = 50)
H[1,10] = 1
H[2,25] = 1
H[3,30] = 1
sigma_e = 0
sigma_dr = diag(3)*sigma_e
mu_rd_zero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_zero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_zero = sqrt(diag(sigma_rd_zero))
std_rd_zero[30] = 0
std_rd_zero[25] = 0
std_rd_zero[10] = 0
lower_bound_zero = mu_rd_zero - qnorm(0.95)*std_rd_zero
upper_bound_zero = mu_rd_zero + qnorm(0.95)*std_rd_zero


df = data.frame(y1 = mu_rd_zero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_zero, y4 = upper_bound_zero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0"))+
  theme(legend.position = 'none')

sigma_e = 0.25
sigma_dr = diag(3)*sigma_e
mu_rd_nonzero = mu_r + 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%(observation-H%*%mu_r)
sigma_rd_nonzero = 5*sigma_r_rho-5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%H%*%sigma_r_rho*5
std_rd_nonzero = sqrt(diag(sigma_rd_nonzero))
lower_bound_nonzero = mu_rd_nonzero - qnorm(0.95)*std_rd_nonzero
upper_bound_nonzero = mu_rd_nonzero + qnorm(0.95)*std_rd_nonzero

df = data.frame(y1 = mu_rd_nonzero, y2 = c(rep(NA,9), observation[1], rep(NA,14), observation[2], rep(NA,4), observation[3], rep(NA,20)), y3 = lower_bound_nonzero, y4 = upper_bound_nonzero)
ggplot(df, aes(x, y = value, color = variable)) + 
    geom_line(aes(y = y1, col = "y1")) + 
    geom_point(aes(y = y2, col = "y2")) + 
    geom_line(aes(y = y3, col = "y3"), linetype = "dashed") + 
    geom_line(aes(y = y4, col = "y4"), linetype = "dashed") +
    scale_colour_manual(labels = c("Prediction", "Observations", "Lower bound", "Upper bound"), values = c("Red", "black", "blue", "green")) + ggtitle(bquote("Prediction of r(x) with 90% prediction interval and "*sigma[epsilon]^2*"=0.25"))+
  theme(legend.title=element_blank())
```

From the plots, we see that with no observation error, the prediction coincides with the observations in the points where we have an observation. Also, in these points there is no uncertainty, so the lower and upper bounds are also equal the observations. When observation error is present, we get uncertainty in the observed points. In addition, adding an observation error leads to larger uncertainty, and thus variance, over the entire prediction.

## e)
We now go on to simulate 100 realizations from the posterior distribution, using the mean and covariance matrix previously computed. Then, we make a prediction based on the 100 samples and create a 90% empirical prediction interval by ordering the realizations and plotting the 5th smallest and largest value for each $x\in L$.

```{r, echo = F, eval = T,out.width = "50%"}
sample_zero = mvrnorm(100, mu_rd_zero, sigma_rd_zero)
prediction_zero = apply(sample_zero,2,mean)
std_empirical_zero = sqrt(apply(sample_zero,2,var))
lower_zero = prediction_zero - qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)
upper = prediction_zero + qt(0.95,99)*std_empirical_zero*sqrt(1+1/100)

rownames(sample_zero) = paste("trial", seq(100), sep="")
colnames(sample_zero) = paste("x", seq(50), sep="")

pred_interval_zero <- apply(sample_zero,2,sort)

dat_zero = as.data.frame(sample_zero)
dat_zero$trial = rownames(dat_zero)
mdat_zero = melt(dat_zero, id.vars="trial")
mdat_zero$x = as.numeric(gsub("x", "", mdat_zero$variable))
#df_zero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_zero, ub = upper_zero)
df_zero = data.frame(x = seq(1,50), pred = prediction_zero, lb = pred_interval_zero[5,], ub = pred_interval_zero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_zero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_zero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_zero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_zero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0"))

sample_nonzero = mvrnorm(100, mu_rd_nonzero, sigma_rd_nonzero)
prediction_nonzero = apply(sample_nonzero,2,mean)
std_empirical_nonzero = sqrt(apply(sample_nonzero,2,var))
lower_nonzero = prediction_nonzero - qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)
upper = prediction_nonzero + qt(0.95,99)*std_empirical_nonzero*sqrt(1+1/100)

rownames(sample_nonzero) = paste("trial", seq(100), sep="")
colnames(sample_nonzero) = paste("x", seq(50), sep="")

pred_interval_nonzero <- apply(sample_nonzero,2,sort)

dat_nonzero = as.data.frame(sample_nonzero)
dat_nonzero$trial = rownames(dat_nonzero)
mdat_nonzero = melt(dat_nonzero, id.vars="trial")
mdat_nonzero$x = as.numeric(gsub("x", "", mdat_nonzero$variable))
#df_nonzero = data.frame(x = seq(1,50), pred = prediction_zerp, lb = lower_nonzero, ub = upper_nonzero)
df_nonzero = data.frame(x = seq(1,50), pred = prediction_nonzero, lb = pred_interval_nonzero[5,], ub = pred_interval_nonzero[95,])

ggplot() +
    theme_bw() +
    theme(panel.grid=element_blank()) +
    geom_line(data = mdat_nonzero, aes(x=x, y=value, group=trial), size=0.2, alpha=0.1) +
    geom_line(data = df_nonzero, aes(x = x, y = pred), size = 1, col = 'red') + 
    geom_line(data = df_nonzero, aes(x = x, y = lb), size = 1, col = 'blue', linetype = "dashed") + 
    geom_line(data = df_nonzero, aes(x = x, y = ub), size = 1, col = 'blue', linetype = "dashed") + 
    ggtitle(bquote("Prediction and 100 realizations of posterior Gaussian RF with "*sigma[epsilon]^2*"=0.25"))
```
From the plots, we again see that with no observation error, the prediction coincides with the observations in the points where we have an observation, and we get zero prediction variance. However, because we here only have simulated values (and only 100 simulations), the variance with observation error is not larger for all other points than the variance without observation error.

Also, we see that the variance and prediction based on the 100 samples coincides well with the theoretical prediction and variance. Still, we observe lower degree of smoothness in the lines, which is as expected when comparing simulated values with theoretical quantities.
## f)
We use the previously generated $n=100$ realizations $\{r_i(x)\}, i \in 1,\dots,n$ with $\sigma_\epsilon^2 = 0$ to provide a prediction $\hat{A}_r$ for the non-linear function on $\{r(x);x\in D\}$,
$$A_r = \int_D I(r(x)>2)dx.$$
The predictor $\hat{A}_r$ will be
$$\hat{A}_r = \frac{1}{n} \sum_{i=1}^n A_{r_i}.$$
We also compute the prediction variance, which will be the sample variance, given as
$$ \text{Var}(\hat{A}_r) = \frac{1}{n-1}\sum_{i=1}^n (A_{r_i}-\hat{A}_r)^2.$$
An alternative predictor is,
$$\tilde{A}_r = \sum_{x\in L} I(\hat{r}(x)>2),$$
with $\hat{r}(x)$ given as in equation \eqref{rhat}. This value is also computed.

```{r, echo = F, eval = T}
extract <- function(sample){
  return(length(sample[which(sample>2)]))
}
A_hat = apply(sample_zero, 1,extract)
cat("A hat: ", mean(A_hat), '\n')

sigma_r_rho = sigmamatrix2
observations = matrix(c(sample2[,10], sample2[,25], sample2[,30]), nrow = 10, ncol = 3)
sigma_e = 0
sigma_dr = diag(3)*sigma_e
r_hat = 5*sigma_r_rho%*%t(H)%*%solve(5*H%*%sigma_r_rho%*%t(H)+sigma_dr)%*%t(observations)
A_tilde = apply(r_hat,2,extract)

cat("A tilde: ", mean(A_tilde), '\n')
cat("Predicted variance of A hat: ", var(A_hat))
```
From the print-out, we see that $\hat{A}_r > \tilde{A}_r$. To shed some light on this result, we make use of Jensen's inequality, which states that for a random variable $X$ and a convex function $\psi$,
$$\psi(E[X]) \leq E[\psi(X)].$$
We then note that $\hat{r}(x) = E[r_i(x)]$, and with $\psi(\xi) = I(\xi >2)$, we get after integrating over $D$ (summing over $L$),
$$\tilde{A}_r = \sum_{x \in L} I(\hat{r}(x)>2)\leq E\Big[\int_D I(r_i(x)>2)dx\Big] = \hat{A}_r.$$
## g)
We now want to sum up our experiences made during this exercise. Firstly, we have investigated and achieved greater understanding as to why correlation functions need to be positive definite. Furhtermore, the choice of correlation function and correlation function parameters has great implications for the structure of the Gaussian RF. By choosing a suitable correlation function, we can impact both smoothness and overall variance of the resulting RF. Including an observation error on the observations also has great impact on the prediction variance in those points, and smaller impact on points far away from the observations. We have also gotten a greater understanding in making predictions on Gaussian RFs, and experienced an application of Jensen's inequality.

# Problem 2: Gaussian RF - real data

## a)
```{r, echo = F, eval = T, out.width = "50%"}
data = read.table("https://www.math.ntnu.no/emner/TMA4250/2017v/Exercise1/topo.dat")
interpolation <- interp(data$x, data$y, data$z)
contour(interpolation, drawlabels = F)
image.plot(interpolation)

#x = rep(interpolation$x,length(interpolation$x))
#y = rep(interpolation$y,each = length(interpolation$y))
#mtrx3d <- data.frame(x = x, y = y, z = as.vector(interpolation$z))
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))
```

## b)
We now want to develop the expression for the minimization problem to be solved for
the universal kriging predictor. We assume the model parameters $\boldsymbol \beta_r^+$ to be unknown, while the parameters ($\sigma_r^2,\eta_r$) are known. Then, we need the kriging predictor $\hat r_0$ to be unbiased, which requires
$$\mathbf{E}[\hat r_0 - r_0] = \boldsymbol \alpha^T\mathbf{E}[\mathbf{r}^d] - \mathbf{E}[r_0] = 0,$$
implying
$$\boldsymbol \alpha^T\mathbf{G}_d\boldsymbol \beta_r^+ = \mathbf{g}_\mathbf{x_0}\boldsymbol \beta_r^+,$$
which finally implies
$$\mathbf{G}_d^T \boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.$$
In addition to being unbiased, we want the predictor to have minimum variance. We ensure minimum variance through the minimum squared-error criterion, which leads to the following minimization problem
\begin{align*}
\hat{\boldsymbol \alpha} &= \arg \min_{\boldsymbol \alpha} \{\mathbf{Var}[\hat r_0 - r_0]\}\\
&= \arg \min_{\boldsymbol \alpha} \{\sigma_r^2 - 2 \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \rho_0 + \boldsymbol \alpha^T \sigma_r^2 \boldsymbol \Sigma_d^\rho \boldsymbol \alpha\}\\
&\text{constrained by: } \mathbf{G}_d^T\boldsymbol \alpha = \mathbf{g}_\mathbf{x_0}.
\end{align*}
This is a quadratic optimization problem with linear constraints. Thus, it is possible to solve it analytically, giving the universal Kriging predictor and associated prediction variance,
\begin{align*}
\hat r_0&=\hat{\boldsymbol \alpha}^T\mathbf{r}^d\\
\sigma_{\hat r}^2 &= \sigma_r^2[1-2\hat{\boldsymbol \alpha}^T\boldsymbol \rho_0 + \hat{\boldsymbol \alpha}^T\boldsymbol \Sigma_d^\rho \hat{\boldsymbol \alpha}],
\end{align*}
with 
$$\hat{\boldsymbol \alpha} = [\boldsymbol \Sigma_d^\rho]^{-1} \Big[\boldsymbol \rho_0 - \mathbf{G}_d^T\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\mathbf{G}_d^T\big]^{-1}\big[\mathbf{G}_d[\boldsymbol \Sigma_d^\rho]^{-1}\boldsymbol \rho_0 - \mathbf{g}_\mathbf{x_0}\big]\Big].$$
## c)
We now let the reference variable $\mathbf{x} \in D \subset \mathbb{R}^2$ be denoted $\mathbf{x} = (x_v,x_h)$ and set $n_g = 6$. Then we define the set of known polynomial functions $\mathbf{g}(\mathbf{x})$ to be all polynomial $x_v^k x_h^l$ for $(k,l) \in \{(0,0),(1,0),(0,1),(1,1),(2,0),(0,2)\}$. The resulting $n_g$-vector $\mathbf{g}(\mathbf{x})$ will be
$$\mathbf{g}(\mathbf{x}) = \{1,x_v, x_h, x_vx_h, x_v^2, x_h^2\}.$$
This gives the following expectation value for $r(\mathbf{x})$
$$\mathbf{E}(r(\mathbf{x})) = \mathbf{g}(\mathbf{x})^T\boldsymbol \beta_r = \beta_1 + x_v \beta_2 + x_h \beta_3 + x_v x_h \beta_4 + x_v^2 \beta_5 + x_h^2 \beta_6.$$
```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# 2nd
kriging_2nd <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd$predict, nrow = 315))

# 1st
kriging_1st <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100)), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st$predict, nrow = 315))

```
## d)
Since the random field is Gaussian, the best linear predictor (our kriging predictor) coencides with the conditional expectation, $E(r_0 | \mathbf{r^d})$. The Gaussian distribution is closed under conditioning, and it follows that $$r_0 | \mathbf{r^d} \sim \phi(r_0; \hat{r}, \sigma^2_{\hat{r}}),$$ where $\sigma^2_{\hat{r}}$ is as previously defined. 

```{r, echo = F, eval = T}
# index of (100, 100)
idx = 100 + 100 * 315
mu = kriging_2nd$predict[idx]
var = kriging_2nd$krige.var[idx]

cat("Probability of r((100, 100)) being larger than 700m: ", pnorm((700 - mu) / sqrt(var), lower.tail = FALSE), '\n')

cat("Elevation at which the probability of r((100, 100)) being smaller is 0.9: ", qnorm(0.9, mean = mu, sd = sqrt(var), lower.tail = TRUE))

```

## e)

```{r, echo = F, eval = T, out.width = "50%"}
locations = expand.grid(seq(1,315,l=315), seq(1,315,l=315))
loc = as.data.frame(locations)
loc$z = rep(0,length(loc$Var1))

# nugget = 5

# 2nd
kriging_2nd_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_5$predict, nrow = 315))

# 1st
kriging_1st_5 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 5), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_5$predict, nrow = 315))

# nugget = 25

# 2nd
kriging_2nd_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("2nd", as.geodata(data)), trend.l = trend.spatial("2nd", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

x = seq(1, 315, l = 315)
y = seq(1, 315, l = 315)
image.plot(x = x, y = y, z = matrix(kriging_2nd_25$predict, nrow = 315))

# 1st
kriging_1st_25 <- krige.conv(coords = t(rbind(data$x,data$y)), data = data$z, locations = locations, krige = krige.control(type.krige = "ok",trend.d = trend.spatial("1st", as.geodata(data)), trend.l = trend.spatial("1st", as.geodata(loc)), cov.model = "exponential", cov.pars = c(2500,100), nugget = 25), output = output.control(messages = F)) 

image.plot(x = x, y = y, z = matrix(kriging_1st_25$predict, nrow = 315))

```

## f)

# Problem 3: Parameter estimation
We consider the stationary Gaussian RF $\{r(\mathbf{x})$; $\mathbf{x} \in D \subset \mathbb{R}^2\}$ with $D: [(1,30), (1,30)]$, with 
\begin{align*}
E\{r(\mathbf{x})\} &= \mu_r = 0\\
Var\{r(\mathbf{x})\} &= \sigma_r^2\\
Coor\{r(\mathbf{x}), r(\mathbf{x}')\} &= \exp \{-\tau/\xi_r\},
\end{align*}
with $\tau = |\mathbf{x} - \mathbf{x}'|$.

## a)
We discretize the Gaussian RF to get $\{r(x); x \in L\}$ on a grid $L : [30\times 30] \in D$. The model parameters are set to $\sigma_r^2 = 2$ and $\xi_r = 15$. Then we generate one realization of the discretized Gaussian RF and display it.

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
x = rep(seq(1, 30, length.out = 30),30)
y = rep(seq(1, 30, length.out = 30),each = 30)
dat = matrix(c(x,y), nrow = 900, ncol = 2)
distances = dist(dat)
distances <- as.matrix(distances)
sigma_r = 2
xi_r = 15
corr <- function(x, xi){
  return(exp(-x/xi))
}
corrmatrix = corr(distances, xi_r)
covmatrix = sigma_r*corrmatrix
mu_r = rep(0,900)

sample = mvrnorm(1,mu_r,covmatrix)

#mtrx3d <- data.frame(x = x, y = y, z = sample)
#mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#ggplot(mtrx.melt, aes(x = x, y = y, z = value)) +
#         geom_raster(aes(fill = value))

x = seq(1,30)
y = seq(1,30)

image.plot(x = x, y = y, z = matrix(sample, ncol = 30))
```
From the display, it is reasonable to assume that the generated realization comes from a Gaussian RF. 

## b)
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
vario <- variog(messages = F, coords = dat, data = sample)
df <- data.frame(x = seq(0,40))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario$v, u = vario$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram")
```


## c)


```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
n_obs = 36
obs36 <- unique(ceiling(runif(n_obs,0,900)))
while(length(obs36)<n_obs){
  obs36 <- unique(c(obs36,ceiling(runif(1,0,900))))
}
x_loc36 <- obs36%%30
y_loc36 <- obs36%/%30+1
coordinates = matrix(c(x_loc36,y_loc36), nrow = n_obs, ncol = 2)

datapoints = sample[obs36]
vario2 <- variog(messages = F,coords = coordinates, data = datapoints)
df <- data.frame(x = seq(0,max(as.matrix(dist(coordinates)))))
df$y = sigma_r*(1-corr(df$x, xi_r))
df2 <- data.frame(v = vario2$v, u = vario2$u)
ggplot() + 
  geom_line(data = df, aes(x = x, y = y, colour = "Theoretical")) +  
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of empirical and theoretical variogram")
```


```{r, echo = F, eval = T, out.width = "50%", fig.align = "center"}
parameters <- likfit(messages = F, coords = dat, data = sample, ini.cov.pars = c(1,10), cov.model = "exponential")
parameters2 <- likfit(messages = F, coords = coordinates, data = datapoints, ini.cov.pars = c(1,10), cov.model = "exponential")

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters2$cov.pars[1]*(1-corr(df$x,parameters2$cov.pars[2]))
ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial36")) +
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")
```
## d)

```{r, echo = F, eval = T, out.width = "33%"}
n_obs = 9
obs9 <- unique(ceiling(runif(n_obs,0,900)))
while(length(obs9)<n_obs){
  obs9 <- unique(c(obs9,ceiling(runif(1,0,900))))
} 
x_loc9 <- obs9%%30
y_loc9 <- obs9%/%30+1
coordinates9 = matrix(c(x_loc9,y_loc9), nrow = n_obs, ncol = 2)
datapoints9 = sample[obs9]

parameters9 <- likfit(messages = F, coords = coordinates9, data = datapoints9, ini.cov.pars = c(1,10), cov.model = "exponential")
vario9 <- variog(messages = F, coords = coordinates9, data = datapoints9)
df2 <- data.frame(v = vario9$v, u = vario9$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters9$cov.pars[1]*(1-corr(df$x,parameters9$cov.pars[2]))
v1<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial9")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")

n_obs = 64
obs64 <- unique(ceiling(runif(n_obs,0,900)))
while(length(obs64)<n_obs){
  obs64 <- unique(c(obs64,ceiling(runif(1,0,900))))
} 
x_loc64 <- obs64%%30
y_loc64 <- obs64%/%30+1
coordinates64 = matrix(c(x_loc64,y_loc64), nrow = n_obs, ncol = 2)
datapoints64 = sample[obs64]


parameters64 <- likfit(messages = F,coords = coordinates64, data = datapoints64, ini.cov.pars = c(1,10), cov.model = "exponential")
vario64 <- variog(messages = F, coords = coordinates64, data = datapoints64)
df2 <- data.frame(v = vario64$v, u = vario64$u) 

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters64$cov.pars[1]*(1-corr(df$x,parameters64$cov.pars[2]))
v2<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial64")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")

n_obs = 100
obs100 <- unique(ceiling(runif(n_obs,0,900)))
while(length(obs100)<n_obs){
  obs100 <- unique(c(obs100,ceiling(runif(1,0,900))))
} 
x_loc100 <- obs100%%30
y_loc100 <- obs100%/%30+1
coordinates100 = matrix(c(x_loc100,y_loc100), nrow = n_obs, ncol = 2)
datapoints100 = sample[obs100]

parameters100 <- likfit(messages = F,coords = coordinates100, data = datapoints100, ini.cov.pars = c(1,10), cov.model = "exponential")
vario100 <- variog(messages = F, coords = coordinates100, data = datapoints100)
df2 <- data.frame(v = vario100$v, u = vario100$u)

df <- data.frame(x = seq(0,40))
df$y1 = sigma_r*(1-corr(df$x, xi_r))
df$y2 = parameters$cov.pars[1]*(1-corr(df$x,parameters$cov.pars[2]))
df$y3 = parameters100$cov.pars[1]*(1-corr(df$x,parameters100$cov.pars[2]))
v3<-ggplot() + 
  geom_line(data = df, aes(x = x, y = y1, colour = "Theoretical")) +  
  geom_line(data = df, aes(y = y2, x = x, colour = "Full")) +
  geom_line(data = df, aes(y = y3, x = x, colour = "Partial100")) +
  geom_point(data = df2, aes(y = v, x = u, colour = "Empirical")) + 
  xlab(expression(paste(tau))) + ylab("Variogram") + ggtitle("Comparison of maxlik... and theoretical variogram")
v1
v2
v3
```



